\hyphenation{
Barrow
Binet
Chebyshev
Cholesky
Cramer
Gauss
Hausdorff
Householder
Laplace
Runghe
tras-for-ma-zio-ne
Torricelli
}


\chapter{Analisi di stabilità.}

\begin{defi}
Sia $A$ una matrice, $\lambda_1, \ldots, \lambda_s$ gli autovalori di
$A$, si dice \emph{raggio spettrale} di $A$:
\[
\rho(A) := \max_{1\leq i\leq s}(|\lambda_i|).
\]
\end{defi}

Consideriamo $\|\cdot\|$ l'operatore di norma matriciale naturale (o indotta)
e sia $\lambda$ un generico autovalore di $A$, sia $x$ un autovettore
corrispondente normalizzato ($\|x\| = 1$), quindi
\[Ax = \lambda x.\]
\[\|Ax\| = \|\lambda x\| = |\lambda|\cdot \|x\| = |\lambda|.\]
\[\max \|Aw\| \geq \|w\| = 1,\]
\[\Rightarrow \|A\| \geq |\lambda| \quad \forall \lambda \ \textrm{autovalore
di } A.\]
\[\Rightarrow \|A\| \geq \rho(A).\]

\begin{osse}
I problemi visti fin'ora sono problemi teorici, quando ci si mette nei termini
di modellare il problema al fine di utilizzare uno strumento automatico per
il calcolo della soluzione, causa approssimazione di macchina si risolve un
sistema differente, in cui abbiamo:
\[(A+\delta A)(x+\delta x) = b + \delta b,\]
con $\delta A $ e $\delta b $ matrice e vettore delle perturbazioni del sistema
lineare originario.
\end{osse}

Vediamo come un problema di questo tipo possa comunque essere manipolato per i
nostri fini, nonostante l'approssimazione numerica.

\section{Condizionamento del problema.}
\begin{teo}\label{teo6.2}
Siano $\delta A \in \rr^{n \times n}$ e $\delta b \in \rr^{n}$ rispettivamente
la matrice ed il vettore delle perturbazioni del sistema lineare $Ax = b$ con
$b \neq 0$.

Sia $\|\cdot\|$ la norma matriciale indotta e $\|\delta A\|
\cdot \|A^{-1}\| < 1$. Allora la matrice $A + \delta A$ è non singolare.

Indicata con $x + \delta x$ la soluzione del sistema perturbato $(A+\delta A)
(x+\delta x) = b + \delta b$, si ha che:
\[
\frac{\|\delta x\|}{\|x\|} \leq \frac{\textrm{cond(}A\textrm{)}}
{1 -\textrm{cond(}A\textrm{)}
\cdot  \frac{\|\delta A\|}{\|A\|}}
\left[\frac{\|\delta A\|}{\|A\|} + \frac{\|\delta b\|}{\|b\|}\right].
\]
Dove cond($A$)$\ : = \|A\|\cdot\|A^{-1}\|$ è l'indice di condizionamento.
\end{teo}

\begin{osse}
\[
\frac{\|\delta b\|}{\|b\|}\quad \textrm{e}\quad \frac{\|\delta A\|}{\|A\|},
\]
sono gli errori relativi sui dati, mentre
\[ \frac{\|\delta x\|}{\|x\|} \] è l'errore relativo sul risultato.
\end{osse}

\begin{osse}
Se $\|A\|\cdot\|A^{-1}\| < \frac{1}{2}$ allora:
\[
\frac{\|\delta x\|}{\|x\|} \leq 2 \textrm{cond(}A\textrm{)}
\left[\frac{\|\delta A\|}{\|A\|} + \frac{\|\delta b\|}{\|b\|}\right].
\]
\end{osse}

\begin{osse}
Un numero di condizionamento approssimativamente vicino a 1, è \emph{indice}
del buon condizionamento del problema. Sicuramente un numero di condizionamento
alto \emph{garantisce} il mal condizionamento del problema.
\end{osse}

\begin{osse}
Come si vede dalla definizione, l'errore relativo dipende dal numero di
condizionamento, quanto dal grando di perturbazione che hanno la matrice e il
vettore dei termini noti. Quindi anche avendo un numero di condizionamento
considerato buono, non possiamo essere sicuri che l'errore relativo sul
risultato sia accettabile!
Il numero di condizionamento è legato alla matrice $A$, il vettore $b$ potrebbe
essere assai perturbato.
\end{osse}

\begin{osse}
Il numero di condizionamento inficia fondamentalmente due cose:
\begin{itemize}
\item il residuo $|Ax - b|$ con $x$ soluzione approssimata;
\item la precisione numerica $\|x-y\|_{\infty}$ con $x$ soluzione approssimata
ed $y$ soluzione esatta del problema.
\end{itemize}

\end{osse}

Vediamo ora un primo risultato per dimostrare il teorema \ref{teo6.2}.

\begin{teo}
Sia $A \in \rr^{n \times n}$, $\|\cdot\|$ la norma matriciale indotta, se
$\|A\| < 1$ allora $I + A$ è non singolare e risulta:
\[\|(I+A)^{-1}\| \leq \frac{1}{1 - \|A\|}.\]
\end{teo}
\begin{dimo}
$\|A\| < 1 \Rightarrow \rho(A) \leq \|A\| < 1$.
Allora tutti gli autovalori sono contenuti nel cerchio di raggio minore di
$1$, in particolare $\lambda \neq \pm 1$.

$I + A$ ha autovalori dati da $1 + \lambda \neq 0$, ovvero non ha autovalore
nullo quindi è non singolare.


\[(I + A)(I+A)^{-1} = I,\]
\[(I + A)^{-1} + A(I + A)^{-1} = I,\]
\[(I + A)^{-1} = I - A(I + A)^{-1}.\]

\[\begin{array}{lcl}
\|(I + A)^{-1}\| & = & \|I - A(I + A)^{-1}\| \\
& \leq & \|I\| + \|A(I + A)^{-1}\| \\
& \leq & 1 + \|A\| \cdot \|(I + A)^{-1}\|.
\end{array}\]

$\Rightarrow \|(I + A)^{-1}\|(1 - \|A\|) \leq 1.$\\

$\Rightarrow \|(I + A)^{-1}\| \leq \frac{1}{1 - \|A\|}.$
\end{dimo}

\begin{dimo}Teorema \ref{teo6.2}.
$A + \delta A$ è non singolare.

\[A + \delta A = A(I + A^{-1} \delta A).\]
$I + A^{-1} \delta A$ è non singolare poiché $\|A^{-1}\delta A\| \leq \|\delta
A\|\cdot\|A^{-1}
< 1\|$ per il teorema precedente. Allora $A + \delta A$ è
non singolare essendo prodotto di matrici non singolari. Inoltre:
\[\|(I + A^{-1}\delta A)^{-1}\|\leq\frac{1}{1 - \|A^{-1}\delta A\|}
\leq \frac{1}{1 - \|A^{-1}\|\cdot\|\delta A\|}.\]
\[
(A + \delta A)(A + \delta A)^{-1} = I.
\]

\[(A + \delta A)(x + \delta x)=(b + \delta b); \qquad Ax = b.\]

\[ (A + \delta A)(x + \delta x) - Ax = (b + \delta b) -b.\]

\[ \cancel{Ax} + A\delta x + \delta Ax + \delta A\delta x
\cancel{- Ax} = \cancel{b} + \delta b \cancel{-b}.\]

\[(A + \delta A)\delta x = \delta b - \delta Ax.\]

\[
\begin{array}{lcl}
\delta x & = & (A + \delta A)^{-1}\delta b - (A + \delta A)^{-1}\delta Ax \\
         & = & [A(I + A^{-1}\delta A)]^{-1}\delta b -
               [A(I + A^{-1}\delta A)]^{-1}\delta Ax \\
         & = & (I + A^{-1}\delta A)^{-1}A^{-1}\delta b -
               (I + A^{-1}\delta A)^{-1}A^{-1}\delta Ax.
\end{array}\]

\[
\begin{array}{lcl}
\|\delta x\| & \leq & \|(I + A^{-1}\delta A)^{-1}\|\cdot \|A^{-1}\| \cdot
               \|\delta b\| + \|(I + A^{-1}\delta A)^{-1}\| \cdot \|A^{-1}\|
               \cdot \|\delta A\|\cdot\|x\|  \\
             & \leq & \frac{1}{1 - \|A^{-1}\|\cdot\|\delta A\|} \|A^{-1}\|
             (\|\delta b\| + \|\delta A\|\cdot\|x\|).
\end{array}
\]

\[
\frac{\|\delta x\|}{\|x\|} \leq
\frac{\|A^{-1}\|}{1 - \|A^{-1}\|\cdot\|A\|\cdot \frac{\|\delta A\|}{\|A\|}}
\cdot \left[ \frac{\|\delta b\|}{\|x\|} + \frac{\|\delta A\|}{\|A\|}
\|A\|\right].
\]

Da $Ax = b$ segue che:

\[\|A\|\cdot \|x\| \geq \|Ax\| = \|b\| \ \Rightarrow \ \|x\| \geq
\frac{\|b\|}{\|A\|}.\]
\[\frac{\|\delta x\|}{\|x\|} \leq \frac{\textrm{cond(}A\textrm{)}}
{1 -\textrm{cond(}A\textrm{)}
\cdot  \frac{\|\delta A\|}{\|A\|}}
\left[\frac{\|\delta A\|}{\|A\|} + \frac{\|\delta b\|}{\|b\|}\right].\]
\end{dimo}

