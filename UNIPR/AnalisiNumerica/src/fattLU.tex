\hyphenation{
Barrow
Binet
Cramer
Gauss
Hausdorff
Laplace
Runghe
tras-for-ma-zio-ne
Torricelli
}

\chapter{Fattorizzazione LU.}
\begin{defi}
Sia $A_1 \equiv A$, $Ax = b$, con $A \in \rr^{n \times n}$, il solito problema
lineare, dal capitolo precedente sappiamo esistere un algoritmo che trasforma
la matrice quadrata di ordine $n$, $A$ in una equivalente di tipo triangolare
superiore:
\[E_{n-1}P_{n-1,j} \cdots E_{2}P_{2,j}E_{1}P_{1,j}A_{1} = A_n.\]
Allora $A_n \equiv U$.
\end{defi}

\begin{osse}
Se $P_{j,j} \equiv I$ (ovvero $a_{j,j} \neq 0$) per $j = 1, \ldots, n-1$
allora si ha:
\[E_{n-1} \cdots E_{2}E_{1}A_{1} = U.\]
\end{osse}

\section{Fattorizzazione LU senza permutazioni.}
Come si è dedotto dal teorema \ref{teo3.2}, per ogni matrice elementare
$E_i$ è possibile calcolarne l'inversa, per cui ``lavorando'' sulle matrici
tramite moltiplicazioni riga per colonna da
$E_{n-1} \cdots E_{2}E_{1}A_{1} = U$
si ottiene:

\[
\cancel{E_{n-1}^{-1}}\cancel{E_{n-1}}E_{n-2} \cdots E_{2}E_{1}A_{1} =E_{n-1}^{-1} U,
\]
\[
\cancel{E_{n-2}^{-1}}\cancel{E_{n-2}}\cdots E_{2}E_{1}A_{1} =
E_{n-2}^{-1}E_{n-1}^{-1} U,
\]
\[\vdots\]
\[
\cancel{E_{1}^{-1}}\cancel{E_{1}}
A_{1} =E_{1}^{-1}E_{2}^{-1} \cdots E_{n-2}^{-1}E_{n-1}^{-1} U,
\]

\[A_{1} =
\underbrace{\left(E_{1}^{-1}E_{2}^{-1} \cdots E_{n-2}^{-1}E_{n-1}^{-1} \right)}
_{*}
U.\]

Espandendo $E_{1}^{-1}E_{2}^{-1} \cdots E_{n-2}^{-1}E_{n-1}^{-1}$ ($*$) si ottiene:
\[
\begin{array}{ccl}
* & = &
(I-\alpha_1u_1e_1^t)(I-\alpha_2u_2e_2^t)\cdots(I -\alpha_{n-1}u_{n-1}e_{n-1}^t)\\
\\& = & (I -\alpha_1u_1e_1^t-\alpha_2u_2e_2^t+
\underbrace{\alpha_1u_1e_1^t\alpha_2u_2e_2^t }_{e_1^t(\alpha_2u_2) =\ 0})
(I -\alpha_3u_3e_3^t)\cdots(I -\alpha_{n-1}u_{n-1}e_{n-1}^t)\\
\\& = & (I -\alpha_1u_1e_1^t-\alpha_2u_2e_2^t)
(I -\alpha_3u_3e_3^t)\cdots(I -\alpha_{n-1}u_{n-1}e_{n-1}^t)\\
\\& = & (I -\alpha_1u_1e_1^t-\alpha_2u_2e_2^t-\alpha_3u_3e_3^t
+\underbrace{\cancel{\alpha_2u_2e_2^t\alpha_3u_3e_3^t}}_{e_2^t(\alpha_3u_3) =\ 0}
+\underbrace{\cancel{\alpha_1u_1e_1^t\alpha_2u_2e_2^t}}_{e_1^t(\alpha_2u_2) =\ 0}
\cdots(I -\alpha_{n-1}u_{n-1}e_{n-1}^t\\
\\& = & (I -\alpha_1u_1e_1^t-\alpha_2u_2e_2^t-\alpha_3u_3e_3^t)
\cdots (I -\alpha_{n-1}u_{n-1}e_{n-1}^t) \\
\\& = & I -\alpha_1u_1e_1^t-\alpha_2u_2e_2^t-\alpha_3u_3e_3^t -
\cdots -\alpha_{n-1}u_{n-1}e_{n-1}^t.
\end{array}
\]
Passando alla notazione matriciale:

\[
* =
\left[\begin{array}{ccc}
1 & & \\
& \ddots & \\
& & 1
\end{array}\right]-
\left[\begin{array}{ccc}
0 & \cdots & 0 \\
m_{2,1} & & 0 \\
\vdots & & \vdots \\
m_{n,1} & & 0
\end{array}\right]-
\left[\begin{array}{cccc}
0 & 0 & \cdots & 0 \\
0 & 0 & \cdots & 0 \\
0 & m_{3,2} & &\vdots \\
\vdots & \vdots & &\\
0 & m_{n,2} & & 0
\end{array}\right] -\ \cdots
\]

\[\cdots \ -
\left[\begin{array}{cccc}
0 & \cdots & 0 & 0 \\
\vdots &\ddots & \vdots & \vdots\\
& & 0 & \vdots \\
0 &  & m_{n,n-1} & 0
\end{array}\right]=
\left[\begin{array}{ccccc}
1 & 0 & 0 & \cdots & 0 \\
-m_{2,1} & 1 & 0 & \cdots & 0 \\
-m_{3,1} & -m_{3,2} & 1 & & 0 \\
\vdots & \vdots & \ddots & \ddots & \vdots\\
-m_{n,1} & -m_{n,2} & \cdots & -m_{n,n-1} & 1
\end{array}\right] = L.
\]
$L$ è quindi una matrice triangolare inferiore. Si può dunque vedere la
matrice $A$ come il prodotto di $L$ ed $U$.
\[A = LU.\]

\begin{osse}
Dato il problema $Ax = b$, per quanto si è visto, è possibile riscriverlo
come:
\[LUx = b.\]
Sia ora $Ay = c(x)$ un sistema lineare i cui termini noti siano in funzione
delle soluzione del sistema precedente. Come visto nell'osservazione \ref{Gauss-inline},
vorremmo risolvere entrambi i sistemi lineari in un colpo solo, risparmiando conti. Dato
il legame che unisce la soluzione del primo sistema con la soluzione del secondo, questo
non è immediatamente realizzabile. Tuttavia se sappiamo esistere una fattorizzazione $LU$
della nostra matrice $A$, allora possiamo scrivere:
\[A = LU, \quad L\underbrace{Ux}_{z} = b.\]
Che equivale a risolvere il seguente sistema:
\[
\left\{\begin{array}{l}Lz = b \\ Ux = z\end{array}\right..
\]
Una volta ricavata la soluzione di questo sistema (tramite sostituzioni in
avanti e indietro) è sufficiente risolvere il sistema seguente:
\[L\underbrace{Uy}_{t} = c(x),\]
\[
\left\{\begin{array}{l}Lt = c(x) \\ Uy = t\end{array}\right..
\]
Il costo della risoluzione del problema $Ay = c(x)$ è quindi O$(n^2)$.
In questo modo abbiamo utilizzato la fattorizzazione $LU$ una sola volta,
risolvendo poi sistemi lineari a costo inferiore.
\end{osse}

\section{Fattorizzazione LU con permutazioni.}
Partiamo dall'ipotesi che, utilizzando l'algoritmo di Gauss per ottenere
la matrice triangolare superiore $U$, si ``incontrino'' elementi diagonali
nulli. Come visto in precedenza è possibile procedere con l'algoritmo
invertendo l'ordine delle righe mediante l'utilizzo di matrici di permutazione.
Ma cosa accade alla matrice $E_{n-1}P_{n-1,} \cdots E_{2}P_{2,}E_{1}P_{1}$? E'
ancora una matrice triangolare inferiore?

\begin{exe}
Caso semplice:
\[
A_1 = A =
\left[\begin{array}{cccc}
a_{1,1} & a_{1,2} & a_{1,3} & a_{1,4} \\
a_{2,1} & a_{2,2} & a_{2,3} & a_{2,4} \\
a_{3,1} & a_{3,2} & a_{3,3} & a_{3,4} \\
a_{4,1} & a_{4,2} & a_{4,3} & a_{4,4}
\end{array}\right].
\]
\begin{equation}\label{eq4.1}E_3P_3E_2P_2E_1P_1A_1 = U.\end{equation}

\[E_1 =
\left[\begin{array}{ccccc}
1      & 0 & 0 &  0 \\
m_{2,1} & 1 & 0 &  0 \\
m_{3,1} & 0 & 1 &  0 \\
m_{4,1} & 0 & 0 &  1
\end{array}\right],
\]
\[P_2E_1 =
\left[\begin{array}{ccccc}
1      & 0 & 0 &  0 \\
m_{4,1} & 0 & 0 &  1 \\
m_{3,1} & 0 & 1 &  0 \\
m_{2,1} & 1 & 0 &  0
\end{array}\right].
\]
In questo esempio $P_2$ inverte la seconda con la quarta riga. Come si vede
non risulta più una matrice triangolare inferiore.
\end{exe}

Utilizzando la seguente proprietà delle matrici elementari:
\[P_{i,j}P_{i,j} = I.\]
E' Possibile quindi rivedere la (\ref{eq4.1}) come:
\[E_3P_3E_2P_2E_1IP_1A_1 = U, \quad I = P_2P_2.\]
Allora: $E_3P_3E_2P_2E_1P_2P_2P_1A_1 = U.$

\[
E_3P_3E_2(P_2E_1P_2)P_2P_1A_1 = U.
\]

\[E_1^{(1)} = (P_2E_1)P_2 =
\left[\begin{array}{ccccc}
1      & 0 & 0 &  0 \\
m_{4,1} & 1 & 0 &  0 \\
m_{3,1} & 0 & 1 &  0 \\
m_{2,1} & 0 & 0 &  1
\end{array}\right].
\]
Tale matrice è nuovamente di tipo triangolare inferiore, poichè $P_2E_1$
scambia la seconda con la quarta riga, mentre $(P_2E_1)P_2$ scambia la seconda
con la quarta colonna.

Riscriviamo la (\ref{eq4.1}):
\[E_3P_3E_2E_1^{(1)}P_2P_1A_1 = U.\]
\[E_3P_3E_2IE_1^{(1)}IP_2P_1A_1 = U.\]
\[E_3(P_3E_2P_3)(P_3E_1^{(1)}P_3)P_3P_2P_1A_1 = U.\]

Poste:
\[E_2 =
\left[\begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & m_{3,2} & 1 & 0 \\
0 & m_{4,2} & 0 & 1
\end{array}\right],
\quad P_3E_2 =
\left[\begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & m_{4,2} & 0 & 1 \\
0 & m_{3,2} & 1 & 0
\end{array}\right].
\]
Ora $P_3$ inverte la terza con la quarta riga. Come si vede
non risulta più una matrice triangolare inferiore.

\[(P_3E_2)P_3 =
\left[\begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & m_{4,2} & 1 & 0 \\
0 & m_{3,2} & 0 & 1
\end{array}\right] = E_2^{(1)}.
\]

\[P_3E_1^{(1)}P_3  =
\left[\begin{array}{ccccc}
1      & 0 & 0 &  0 \\
m_{4,1} & 1 & 0 &  0 \\
m_{3,1} & 0 & 1 &  0 \\
m_{2,1} & 0 & 0 &  1
\end{array}\right]= E_1^{(2)}.
\]
Ne segue che l'equazione si può riscrivere come:

\[
\underbrace{E_3E_2^{(1)}E_1^{(2)}}_{mat. ele.}P_3P_2P_1A_1 = U.
\]

\[P_3P_2P_1A_1 = E_1^{(2)^{-1}}E_2^{(1)^{-1}}E_3^{-1}U,\]

ovvero: $PA = LU$.

\begin{flushleft}
Possiamo generalizzare questi conti nella seguente:
\end{flushleft}

\begin{defi}
Data una matrice $A$ non singolare, \emph{esiste sempre} una matrice di
permutazione $P$ tale che il prodotto $PA$ è fattorizzabile da due matrici, una
triangolare inferiore ed una triangolare superiore (U data dall'algoritmo di Gauss e L
a diagonale unitaria avente elementi opposti agli elementi $m_{i,j}$ di eliminazione).
\end{defi}

\section{Fattorizzazione generalizzata.}
Siano $P_i$ matrici generiche di permutazione, per $i = 1, \ldots, n-1$ (può
ivi comparire anche
la matrice identica $I$), possiamo quindi generalizzare la fattorizzazione
$LU$, a partire dall'algoritmo di Gauss,
come segue:

\[E_{n-1}P_{n-1,} \cdots E_{2}P_{2,}E_{1}P_{1,}A_1 = U.\]
$PA = LU \longrightarrow P = P_{n-1}P_{n-2}\cdots P_2P_1.$\\

Primo passo:
\[E_{n-1}P_{n-1,} \cdots E_{2}\underbrace{P_{2,}E_{1}P_{2,}}_{E_1^{(1)}}
P_{2,}P_{1,}A_1 = U.\]

Secondo passo:
\[E_{n-1}P_{n-1,} \cdots E_3\underbrace{P_3E_2P_3}_{E_2^{(1)}}
\underbrace{P_3E_1^{(1)}P_3}_{E_1^{(2)}}P_3P_2P_1A_1 = U.\]

Terzo passo:
\[E_{n-1}P_{n-1,} \cdots E_4\underbrace{P_4E_3P_4}_{E_3^{(1)}}
\underbrace{P_4E_2^{(1)}P_4}_{E_2^{(2)}}
\underbrace{P_4E_1^{(3)}P_4}_{E_1^{(3)}}P_4P_3P_2P_1A_1 = U.\]

$k$-esimo passo
\[E_{n-1}P_{n-1,} \cdots E_{k+1}\underbrace{P_{k+1}E_kP_{k+1}}_{E_{k}^{(1)}}
E_{k-1}^{(2)}E_{k-2}^{(3)}\cdots E_2^{(k-1)}
E_1^{(k)}P_{k+1}\cdots P_4P_3P_2P_1A_1 = U.\]

Abbiamo visto che a meno di una matrice $P$ di permutazione, la fattorizzazione
$LU$ è sempre possibile. E' chiaro che se durante l'algoritmo di Gauss non si fa
mai uso di scambi di righe\footnote{Ad esempio: l'ipotesi $a_{k,k}^(k) \neq 0$
è sempre verificata, e in generale non mi interessa fare pivoting} o di colonne,
non si utilizzano matrici di permutazione, quindi $P=I$. Quindi per sapere se per qualche
$j$ (che si può vedere come passo dell'algoritmo di Gauss) $P_j = I$, occorre
eseguire ed analizzare tutti i passi? Cosa è possibile analizzare su $A$?
In questo senso ci viene in aiuto il seguente:

\begin{teo}
\label{teo4.4}
Sia $A$ una matrice di ordine $n$ non singolare tale che tutti i minori
principali $\Delta_k \ (k = 1, \ldots, n)$ siano invertibili.
\[\Delta_k = \left[
\begin{array}{ccc}
a_{1,1}&\cdots&a_{1,k}\\
\vdots& & \vdots \\
a_{n,1}&\cdots&a_{k,k}
\end{array}\right].\]
Allora esiste una matrice $L$ triangolare inferiore con $l_{i,i} = 1, \ (i =
1, \ldots, n)$ e una matrice triangolare superiore $U$ tale che:
\[A = LU.\]
Inoltre tale fattorizzazione è unica.
\end{teo}
%%% ----Dimostrazione
\begin{dimo}Sia $\Delta_k \subseteq A_1$ non singolare, per $k = 1, \ldots, n$.
\[
A_1 = \left[
\begin{array}{ccccc}
a_{1,1}^{(1)} & \cdots & a_{1,k}^{(1)}  & \cdots   & a_{1,n}^{(1)}\\
\vdots      & \ddots &   \vdots     &          & \vdots     \\
a_{k,1}^{(1)} & \cdots & a_{k,k}^{(1)}  & \cdots   & a_{k,n}^{(1)}\\
\vdots      &        & \vdots       & \ddots   & \vdots     \\
a_{n,1}^{(1)} & \cdots & a_{n,k}^{(1)}  & \cdots   & a_{n,n}^{(1)}
\end{array} \right].
\]
det$\Delta_k \neq 0$.
\[\Delta_1 = [a_{1,1}^{(1)}], \quad \Delta_2 = \left[\begin{array}{cc}
a_{1,1}^{(1)} & a_{1,2}^{(1)}\\
a_{2,1}^{(1)} & a_{2,2}^{(1)}
\end{array}\right].\]
Si dimostra per induzione su $k$.
\begin{itemize}
\item[-]Caso base: $k = 1$.\\

det$\Delta_1 \neq 0 \Rightarrow a_{1,1}^{(1)} \neq 0$, dunque $P_1 \equiv I$.

\item[-]Passo induttivo:\\
Ipotesi induttiva: $P_{k-1} = P_{k-2}\cdots P_{2} = P_1 = I$.

Tesi: $P_k \equiv I$.

Tramite il passo induttivo si è costruito  $A_k = (E_{k-1}\cdots E_1)A_1$:
\[
A_k =
\left[
\begin{array}{ccccc}
1      & 0      & 0      & \cdots & 0 \\
*      & 1      & 0      & \cdots & 0 \\
*      & *      & 1      &        & \vdots \\
\vdots & \vdots &        & \ddots & 0  \\
*      & *      & \cdots & *      & 1
\end{array} \right]
\left[
\begin{array}{ccccc}
a_{1,1}^{(1)} & \cdots & a_{1,k}^{(1)}  & \cdots   & a_{1,n}^{(1)}\\
\vdots      & \ddots &   \vdots     &          & \vdots     \\
a_{k,1}^{(1)} & \cdots & a_{k,k}^{(1)}  & \cdots   & a_{k,n}^{(1)}\\
\vdots      &        & \vdots       & \ddots   & \vdots     \\
a_{n,1}^{(1)} & \cdots & a_{n,k}^{(1)}  & \cdots   & a_{n,n}^{(1)}
\end{array} \right]=
\]

\[
= \left[
\begin{array}{cccccc}
a_{1,1}^{(1)} &\cdots       &a_{1,k-1}^{(1)} &a_{1,k}^{(1)} &\cdots &a_{1,n}^{(1)}\\
0           & a_{2,2}^{(2)} &\cdots        &a_{2,k}^{(2)} &\cdots &a_{2,n}^{(2)}\\
\vdots      &             &\ddots        &\vdots      &       &\vdots\\
0           &             &              &a_{k,k}^{(k)} &\cdots &a_{k,n}^{(k)}\\
\vdots      &             &              &\vdots      &       &\vdots \\
0           &             &              &a_{n,k}^{(k)} &\cdots &a_{n,n}^{(k)}
\end{array} \right].
\]
Possiamo vedere $A_k$ come una matrice a blocchi:
\[
\left[
\begin{array}{c|c}
A_{1,1}^{(k)} & A_{1,2}^{(k)} \\
\hline
A_{2,1}^{(k)} & A_{2,2}^{(k)}
\end{array}
\right] =
\left[
\begin{array}{c|c}
B_{1,1}^{\ } & B_{1,2} \\
\hline
B_{2,1}^{\ } & B_{2,2}
\end{array}
\right]
\left[
\begin{array}{c|c}
A_{1,1}^{(1)} & A_{1,2}^{(1)} \\
\hline
A_{2,1}^{(1)} & A_{2,2}^{(1)}
\end{array}
\right].
\]
Come si ottiene $A_{1,1}^{(k)}$? Prodotto righe per colonne.
\[
\left[
\begin{array}{c|c}
\\
  B_{1,1}A_{1,1}^{(1)} + \cancel{B_{1,2}A_{2,1}^{(1)}}
& B_{1,1}A_{1,2}^{(1)} + B_{1,2}A_{2,2}^{(1)} \\
\\
\hline
\\
  B_{2,1}A_{1,1}^{(1)} + B_{2,2}A_{2,1}^{(1)}
& B_{2,1}A_{1,2}^{(1)} + B_{2,2}A_{2,2}^{(1)} \\
&
\end{array}
\right].
\]
Si noti che $B_{1,2}$ è la matrice nulla.

Si ha dunque che $A_{1,1}^{(k)} = B_{1,1}A_{1,1}^{(1)}$, i rimanenti blocchi non
sono utili, poiché occorrono solo elementi che stanno sulla diagonale
principale.

\[\textrm{det}\left(A_{1,1}^{(k)}\right)
= \textrm{det}(B_{1,1})\textrm{det}\left(A_{1,1}^{(1)}\right).
\qquad \textrm{(Binet)}\]
\[
\prod_{i = 1}^{k}a_{i,i}^{(i)} = 1 \cdot \textrm{det}(\Delta_k) \neq 0.
\]
Allora:
\[a_{i,i}^{(i)} \neq 0 \qquad i = 1,\ldots , k.\]
Ovvero per ogni $i$ si ha quindi che $P_i = I$. Questo dimostra l'esistenza
di due matrici di tipo triangolare inferiore e triangolare superiore che
fattorizzano $A$. Non resta che dimostrarne l'unicità.
\end{itemize}

Sia $A$ una matrice di ordine $n$ non singolare.
Siano $L$ una matrice triangolare inferiore ed $U$ una matrice triangolare
superiore di ordine $n$ tali che:
\[A = LU.\]
Per quanto dimostrato sopra sappiamo che esistono.

\begin{flushleft}
Ipotesi: $(L)_{i,i} = 1, \quad  1 \leq i \leq n$.
\end{flushleft}
Siano ora $L_1$, $L_2$ e $U_1$, $U_2$ tali che:
\[A =  L_1U_1,\]
\[A  = L_2U_2,\]
\[
(L_1)_{i,i} = (L_2)_{i,i} = 1, \quad i = 1,\ldots, n.
\]

\begin{flushleft}
Tesi: $L_1U_1 = L_2U_2.$
\end{flushleft}

\[
L_2^{-1}L_1U_1 = L_2^{-1}L_2U_2
              = IU_2.
\]
\[L_2^{-1}L_1 = U_2U_1^{-1}. \]

\[
L_2^{-1}=
\left[
\begin{array}{ccccc}
1      & 0      & 0      & \cdots & 0 \\
*      & 1      & 0      & \cdots & 0 \\
*      & *      & 1      &        & \vdots \\
\vdots & \vdots &        & \ddots & 0  \\
*      & *      & \cdots & *      & 1
\end{array} \right]
\stackrel{\textrm{infatti}}{\longrightarrow}
\left\{\begin{array}{lr}
L_2\overline{a}_1 = e_1 & \\
\vdots & \textrm{Ma $L_2$ è già triangolare.} \\
L_2\overline{a}_n = e_n
\end{array}\right.
\]
\[
L_2^{-1}L_1 =
\left[
\begin{array}{ccccc}
1      & 0      & 0      & \cdots & 0 \\
*'      & 1      & 0      & \cdots & 0 \\
*'      & *'      & 1      &        & \vdots \\
\vdots & \vdots &        & \ddots & 0  \\
*'      & *'      & \cdots & *'      & 1
\end{array} \right].
\]

\[
(L_2^{-1}L_1)_{i,j} = \left\{\begin{array}{lr}
0  & j > i \\
1  & j = i \\
*' & \textrm{altrimenti.}
\end{array}\right.
\]
Come risulta $U_2U_1^{-1}$?
\begin{flushleft}
Come si calcola $U_1^{-1}$?\\
$U_1x = e_1$, $U_2x = e_2, \ldots U_nx = e_n$ come nell'esercizio \ref{ese2}.
\end{flushleft}
\[
U_1^{-1}=
\left[
\begin{array}{ccccc}
*      & *      & *      & \cdots & * \\
0      & *      & *      & \cdots & * \\
0      & 0      & *      & \cdots & * \\
\vdots & \vdots &        & \ddots & \vdots  \\
0      & 0      & \cdots & 0      & *
\end{array} \right].
\]

\[L_2^{-1}L_1 = U_2U_1^{-1} =
\left[
\begin{array}{ccccc}
1      & 0      & 0      & \cdots & 0 \\
*'      & 1      & 0      & \cdots & 0 \\
*'      & *'      & 1      &        & \vdots \\
\vdots & \vdots &        & \ddots & 0  \\
*'      & *'      & \cdots & *'      & 1
\end{array} \right] =
\left[
\begin{array}{ccccc}
*      & *      & *      & \cdots & * \\
0      & *      & *      & \cdots & * \\
0      & 0      & *      & \cdots & * \\
\vdots & \vdots &        & \ddots & \vdots  \\
0      & 0      & \cdots & 0      & *
\end{array} \right].
\]
Per ottenere l'uguaglianza occorre che gli elementi $*$ al di sopra della
diagonale principale di $U_2U_1^{-1}$ siano tutti nulli come gli elementi $*'$
al di sotto della diagonale principale di $L_2^{-1}L_1$.
Inoltre gli elementi della diagonale di $U_2U_1^{-1}$ devono essere tutti uguali
a $1$. Da questo segue che:
\[L_2^{-1}L_1 = I \quad \wedge \quad U_2U_1^{-1} = I.\]
Ovvero $L_2 = L_1$ e $U_2 = U_1$.
\end{dimo}


Si analizzi ora l'ipotesi che $(L)_{i,i} = c$ con $i = 1, \ldots, n$ e $c \neq
0$. Si ottiene così una differente fattorizzazione, se ne ha una unica
fissando $c$. Se dovesse cadere questa ipotesi?
\begin{center}
$A = LU$ è ancora unica?
\end{center}

\begin{flushleft}
Vediamo un esempio semplice:
\end{flushleft}

\[L =
\left[
\begin{array}{cccc}
l_{1,1} & 0      \\
l_{2,1} & l_{2,2} \\
\end{array} \right]
\qquad U =
\left[
\begin{array}{cccc}
u_{1,1} & u_{1,2} \\
0      & u_{2,2} \\
\end{array} \right]
\]

In questo esempio abbiamo $4$ equazioni in $6$ incognite. Ovvero $ \infty^2 $ possibili
soluzioni. Se fissiamo $l_{1,1} = l_{2,2} = 1$ otteniamo l'unicità.
\begin{flushleft}
Generalizzando:
\end{flushleft}

\[L =
\left[
\begin{array}{cccc}
l_{1,1} & 0      & \cdots & 0 \\
l_{2,1} & l_{2,2} &        & \vdots \\
\vdots & \vdots & \ddots & 0 \\
l_{n,1} & l_{n,2} & \cdots & l_{n,n}
\end{array} \right]
\qquad U =
\left[
\begin{array}{cccc}
u_{1,1} & u_{1,2} & \cdots & u_{1,n} \\
0      & u_{2,2} & \cdots & u_{2,n} \\
\vdots &        & \ddots & \vdots  \\
0      & \cdots & 0      & u_{n,n}
\end{array} \right]
\]
Numero di equazioni: $n^2$.\\
Numero di incognite: $n + (n-1)+ \cdots + 2 + 1 = \frac{n(n+1)}{\cancel{2}}
\cdot \cancel{2} = n^2 + n.$\\
Occorre, quindi, scegliere in modo opportuno i parametri o porre uguale a $1$ gli
elementi diagonali.

Abbiamo quindi visto che nella fattorizzazione $PA=LU$, $P=I$ se e solo se la matrice
$A$ e tutti i suoi minori risultano non singolari. Esiste un'altra proprietà verificabile
che ci consente di fattorizzare $A = LU$ direttamente?

\begin{teo} Sia $A \in \rr^{n \times n}$ non singolare diagonal dominante in senso
stretto, ovvero
\[|a_{i,i}| > \sum_{\substack{j=1 \\ i \neq j}}^{n}|a_{i,j}|, \qquad i = 1, \ldots, n.\]
allora la sua decomposizione $LU$ è ottenibile senza ricorrere a tecniche di pivoting.
\end{teo}

