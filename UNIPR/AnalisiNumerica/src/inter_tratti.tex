\hyphenation{
Barrow
Binet
Chebyshev
Cholesky
Cramer
Gauss
Faber
Hausdorff
Householder
Laplace
Lebesque
Runghe
tras-for-ma-zio-ne
Torricelli
}

\chapter{Interpolazione a tratti.}
Sia $[a,b]$ un intervallo \emph{chiuso} e \emph{limitato}, e $\Delta$
la decomposizione così definita:
\[\Delta = \{a = x_0 < x_1 < \cdots < x_i < \cdots < x_n = b\}.\]
Vogliamo costruire su ciascun tratto $[x_{i-1},x_i]$ della decomposizione 
$\Delta$ per ogni $i = 1, \ldots n$, una funzione lineare che interpola i 
dati $y_{i-1}$ e $y_i$.

\[
\begin{array}{lcl}
S^{(i)}_1 & = & y_{i-1} + f[x_{i-1},x_i](x-x_{i-1}) \\
 & = & \frac{(x-x_{i-1})y_i + (x_i-x)y_{i-1}}{x_i-x_{i-1}}, \qquad x_{i-1} < x 
< x_i. 
\end{array}
\]

\[S_1(x) = \left\{\begin{array}{lcl}
S_1^{(1)}(x) & x_0 \leq x \leq x_1 & S_1^{(1)}(x) = y_0 + f[x_0,x_1](x-x_0) \\
S_1^{(2)}(x) & x_1 \leq x \leq x_2 & S_1^{(2)}(x) = y_1 + f[x_1,x_2](x-x_1) \\
S_1^{(3)}(x) & x_2 \leq x \leq x_3 & S_1^{(3)}(x) = y_2 + f[x_2,x_3](x-x_2) \\
\vdots & & \vdots \\
S_1^{(n)}(x) & x_{n-1} \leq x \leq x_n & S_1^{(n)}(x) = y_2 
+ f[x_{n-1},x_n](x-x_n).
\end{array}\right.
\]

$S_1(x) \in \cc^0([a,b])$.

\begin{osse}
$S_1^{(i)}(x) \in \cc^{\infty}([x_{i-1},x_i])$. Ma la rispettiva funzione
$S_1(x)$ è solo $\cc^0$, ovvero perdiamo di regolarità nei \emph{nodi}
($\notin \cc^1$).
\end{osse}

Prendendo un $x$ interno su un generico tratto, $x \in [x_{i-1},x_i]$ che
errorre commettiamo?
\[e(x) = f(x) - S_1^{(j)}(x).\]

Se $f(x) \in \cc^2([a,b]) \ \longrightarrow \ f \in \cc^2([[x_{i-1},x_i])$.
\[\longrightarrow \ e(x) = f(x) - S_1^{(i)}(x) = \frac{f''(\xi_x)}{2}
(x-x_{i-1})(x-x_i).\]

\[\begin{array}{lcl}\displaystyle
|e(x)| & = & |f(x) - S_1^{(i)}(x)| \\
& = & \frac{|f''(\xi_x)|}{2} \cdot |(x-x_{i-1})(x-x_i)| \\
& \leq & \displaystyle\frac{1}{2}\max_{t \in [x_{i-1},x_i]}|f''(t)| \cdot 
|(x-x_{i-1})(x-x_i)|\\
& \leq & \displaystyle\frac{1}{2}\max_{t \in [x_{i-1},x_i]}|f''(t)|\cdot
\left|\left(\frac{x_{i-1}-x_i}{2} -x_{i-1}\right) \left(
\frac{x_{i-1}}{2}-x_i\right)\right| \\
& = & \displaystyle\max_{t \in [x_{i-1},x_i]}|f''(t)|\frac{1}{8}h_i^2.
\end{array}
\]

\[
\longrightarrow \ \left| f(x) - S_1^{(i)}(x)\right| \leq \max_{t \in [x_{i-1},x_i]}
\left|f''(t)\right|\frac{1}{8}h^2.
\]

\[h = \max_{1 \leq i \leq n} h_i \textrm{ è la norma della decomposizione. 
Ovvero:}\  \max_{1 \leq i \leq n}(x_{i-1}-x_i)\]

\[\forall x \in [a,b], \
|f(x) - S_1(x)| \leq \max_{t \in [a,b]}|f''(t)|\frac{1}{8}h^2 = \|f''(x) \|_\infty
\frac{1}{8}h^2\ \substack{\longrightarrow \\ h \to 0} \ 0.
\]

La logica di questa procedura è quella di prendere un intervallo,
tagliuzzarlo e costruire il polinomio di primo grado. Aumentare i punti
non implica aumentare il grado del polinomio ma costruiamo in questo caso
altri polinomi di primo grado.

Il fatto che l'errore tende a zero per $h$ che tende a zero ci dice che
``raffinando'' la partizione i due grafici vanno a coincidere. In questo caso
si parla di \emph{convergenza}.

Questo è un ottimo risultato, l'unico inconveniente è il numero dei polinomi
di primo grado da calcolare, siamo tuttavia nell'ipotesi $f \in \cc^2$.

Vale un risultato analogo per $f \in \cc^0$? L'oggetto su cui discutere è
$\|f''(x)\|_\infty$ (non l'abbiamo).

\begin{defi}(Modulo di continuità)
Sia $f(x) \in \cc^0([a,b])$, si dice \emph{modulo di continuità} della 
funzione $f$ di parametro $\delta$ una funzione così definita:
\[
\omega(f,\delta) := \max |f(x_1) - f(x_2)|, \quad \forall x_1, x_2 \in [a,b]
\colon |x_1-x_2| < \delta.
\]
\end{defi}

\begin{osse}
\[\lim_{\delta \to 0}\omega(f, \delta) = 0. \qquad 1 = 
\frac{(x-x_{i-1})+(x_i-x)}{h_i}.\]
\end{osse}

\textbf{Tesi:}
\[
|f(x) - S_1(x)| \leq \omega(f, h)\ \substack{\longrightarrow \\ h \to 0} \ 0.
\]

\[
\begin{array}{lcl}
f(x) - S^{(i)}_1(x) & = & 1 \cdot f(x) - \frac{(x-x_{i-1})f(x_i) + 
(x_i-x)f(x_{i-1})}{h_i}^{\phantom{(1)}} \\
& = & \frac{f(x)(x-x_{i-1}) + (x_i-x)}{h_i} -
\frac{(x-x_{i-1})f(x_i)+(x_i-x)f(x_{i-1})}{h_i}^{\phantom{(1)}} \\
& = & \frac{(x-x_{i-1})[f(x)-f(x_i)]-(x_i-x)[f(x)-
f(x_{i-1})]}{h_i}^{\phantom{(1)}}.
\end{array}
\]

\[
\begin{array}{lcl}
\left|f(x) - S^{(i)}_1(x)\right| & \leq & 
\frac{(x-x_{i-1})|f(x)-f(x_i)|-(x_i-x)|f(x)-f(x_{i-1})|}{h_i}^{\phantom{(1)}} \\
& \leq & \frac{(x-x_{i-1})\max\{|f(x)-f(x_i)|,|f(x)-f(x_{i-1})|\}
+(x_i-x) \max\{|f(x)-f(x_i)|,|f(x)-f(x_{i-1})|\}}{h_i}^{\phantom{(1)}} \\
& = &  \cancelto{1}{\frac{(x-x_{i-1})+(x_i-x)}{h_i}}
\max\{|f(x)-f(x_i)|,|f(x)-f(x_{i-1})|\}^{\phantom{(1)}}.
\end{array}
\]

\[
\left|f(x) - S^{(i)}_1(x)\right|  \leq 
\max_{x_i \leq x \leq x_{i-1}}\left\{|f(x)-f(x_i)|,\,|f(x)-
f(x_{i-1})|\right\} \leq \omega(f,h_i).
\]
\[
\left|f(x) - S^{(i)}_1(x)\right|  \leq \omega(f,h_i).
\]
\[
\longrightarrow \ \forall x \in [a,b] \quad |e(x)| =  \left|f(x) - 
S_1(x)\right| \leq \omega(f,h)\ \substack{\longrightarrow \\ h \to 0} \ 0.
\]
\[\textrm{con } h = \max_{1 \leq i \leq n}h_i.\]

Quindi abbiamo ottenuto un risultato analogo per le funzioni continue.

\begin{osse}
Queste tecniche hanno senso se usiamo tabelle con dati precisi, se i dati
fossero affetti da errori sarebbe meglio usare altre tecniche (minimi
quadrati).
\end{osse}

Lo scopo è di semplificare l'oggetto funzione con cui lavoriamo. Abbiamo
scoperto che possiamo farlo con l'unione dei polinomi di primo grado
calcolati sulla partizione dell'intervallo. Il grafico dell'unione dei 
polinomi si sovrappone ad $f$ al tendere a zero della finezza della
partizione.

\begin{exe}
Sia $[a,b]$ il nostro intervallo, $\Delta$ la seguente decomposizione:
\[\Delta = \left\{a = x_0 < x_1 < \cdots < x_n = b\right\}\]
\[\begin{array}{ccccccc}
y_0 & y_1 & \cdots &y_i &\cdots & y_n  \\
y'_0 & y'_1 & \cdots &y'_i &\cdots & y'_n  
\end{array}\]
Posto $[x_{i-1}, x_i] \subset \Delta$ per ogni $i = 1, \ldots, n$, vogliamo 
calcolare $p(x) \in \PP_3$ con $x_{i-1}\leq x \leq x_i$ tale che:
\[
p(x_{i-1}) = y_{i-1}, \quad p(x_i) = y_i,
\]
\[
p'(x_{i-1}) = y'_{i-1}, \quad p'(x_i) = y'_i.
\]
\[
p(x) = a_i + b_i(x-x_{i-1})+ c_i(x-x_{i-1})^2 + d_i(x-x_{i-1})^2(x-x_i).
\]
Usiamo le quattro condizioni.
\[
p'(x) = b_i + 2c_i(x-x_{i-1}) + 2d_i(x-x_{i-1})(x-x_i)+ d_i(x-x_{i-1})^2.
\]
\begin{enumerate}
\item $y_{i-1} = p(x_{i-1}) = a_i$.
\item $y'_{i-1} = p'(x_{i-1}) = b_i$.
\item $y_i = a_i + b_i(x_i-x_{i-1})+ c_i(x_i-x_{i-1})^2$:
\[\longrightarrow \ c_i = \frac{y'_i -b_i(x_i-x_{i-1})}{(x_i-x_{i-1})^2}
= \frac{y_i -a_1 -b_i h_i}{h^2_i}.\]
\item $y'_1 = p(x_i) = b_i + 2c_i h_i + d_i h^2_1$:
\[\longrightarrow \ d_i = \frac{y'_i - b_i - 2 c_i h_i}{h^2_i}.\]
\end{enumerate}

\[
P(x) = \left \{
\begin{array}{lc}
p_3^{(1)}(x) = \ldots & x_0 \leq x \leq x_1, \\
p_3^{(2)}(x) = \ldots & x_1 \leq x \leq x_2,\\
\vdots \\
p_3^{(n)}(x) = \ldots & x_{n-1} \leq x \leq x_n .
\end{array}\right.
\]

$p_3^{(i)}(x) \in \cc^\infty([x_{i-1}, x_i]), \quad \forall i = 1, \ldots,n$.

\[
P(x) \in \cc^1([a,b]).
\]

Sono i polinomi di Hermite generalizzati.
\end{exe}

\section{Spline cubica.}
Vorremmo ottenere una regolarità maggiore di $P$ dell'esempio precedente.

\begin{defi}
Sia $[a,b]$ un intervallo, $\Delta$ la seguente decomposizione:
\[\Delta = \left\{a = x_0 < x_1 < \cdots < x_n = b\right\}\]
\[\begin{array}{ccccccc}
y_0 & y_1 & \cdots &y_i &\cdots & y_n  \\
y'_0 & y'_1 & \cdots &y'_i &\cdots & y'_n  
\end{array}\]
Posto $[x_{i-1}, x_i] \subset \Delta$ per ogni $i = 1, \ldots, n$, $h_i =
x_i - x_{i-1},$
\[h = \max_{1 \leq i \leq n} h_i \ \textrm{la norma della decomposizione;}\]
si dice \emph{spline cubica interpolante} relativa alla decomposizione
$\Delta$ la funzione $S_{3,\Delta}(x)$ tale che:
\begin{enumerate}
\item $S_{3,\Delta}(x)$ è una funzione polinomiale definita a tratti
$[x_{i-1}, x_i], \ i = 1, \ldots, n,$ e su ciascun tratto è di terzo grado.
\item $S_{3,\Delta}(x) \in \cc^2([a,b])$.
\item $S_{3,\Delta}(x_i) = y_i, \quad i = 1, \ldots, n$.
\end{enumerate}
\end{defi}

\[
S_{3,\Delta}(x) = \left \{
\begin{array}{l}
S_{3,\Delta}^{(1)}(x) =  a_0^{(1)} + a_1^{(1)}(x-x_0) +a_2^{(1)}(x-x_0)^2
+ a_3^{(1)}(x-x_0)^3  \\
S_{3,\Delta}^{(2)}(x) =  a_0^{(2)} + a_1^{(2)}(x-x_1) +a_2^{(2)}(x-x_1)^2
+ a_3^{(2)}(x-x_1)^3 \\
\vdots \\
S_{3,\Delta}^{(n)}(x) =  a_0^{(n)} + a_1^{(n)}(x-x_{n-1}) +a_2^{(n)}(x-x_{n-1})^2
+ a_3^{(n)}(x-x_{n-1})^3.
\end{array}\right.
\]
\begin{notabene}
$S_{3,\Delta}^{(1)}(x)$ vale per gli $x \in [x_0, x_1]$, 
$S_{3,\Delta}^{(2)}(x)$  vale per gli $x \in [x_1, x_2]$, etc.
\end{notabene}

Il punto $2$ della definizione allarga la \emph{regolarità} e, chiedendo meno
vincoli, non richiede di conoscere il valore delle derivate prime.

Senza il punto $3$ abbiamo la definizione di \emph{spline} (non interpolante).

\begin{flushleft}
Gradi di libertà: $4n$.\\
Vincoli:\\
\begin{tabular}{rl}
$3(n-1)$: & imporre che i due polinomi contigui si raccordino in modo
continuo.\\
$+ n-1$: 
& imporre a quale quota avviene il raccordo: $S_{3,\Delta}(x_i) = y_i$.\\
$+ 2$: & posto $S_{3,\Delta}(a) = y_0$ e $S_{3,\Delta}(b) = y_n$. \\
$= 4n -2$.
\end{tabular}

Mancano due vincoli, quindi abbiamo $\infty^2$ spline cubiche interpolanti.
\end{flushleft}

Aggiungiamo quindi i due vincoli noi: otteniamo spline differenti a seconda 
dei vincoli aggiunti. Ad esempio aggiungendo le derivate seconde dei due
estremi otteniamo la \emph{spline cubica naturale}.

\begin{defi}
(Spline lineare)\\
$S_{1,\Delta}(x)$:
\begin{enumerate}
\item $S_{1,\Delta}(x)$ è una funzione polinomiale definita a tratti
$[x_{i-1}, x_i], \ i = 1, \ldots, n,$ e su ciascun tratto è di primo grado.
\item $S_{1,\Delta}(x) \in \cc^0([a,b])$.
\item $S_{1,\Delta}(x_i) = y_i, \quad i = 1, \ldots, n$.
\end{enumerate}
\end{defi}

Al tendere di $h$ a zero, la spline lineare tende ad $f$ (convergenza),
sostanzialmente questa spline unisce i punti con delle rette.

\begin{defi}
(Spline di ordine $k$)\\
$S_{k,\Delta}(x)$:
\begin{enumerate}
\item $S_{k,\Delta}(x)$ è una funzione polinomiale definita a tratti
$[x_{i-1}, x_i], \ i = 1, \ldots, n,$ e su ciascun tratto è di grado $k$.
\item $S_{k,\Delta}(x) \in \cc^{k-1}([a,b])$.
\item $S_{k,\Delta}(x_i) = y_i, \quad i = 1, \ldots, n$.
\end{enumerate}
\[
S_{k,\Delta}(x) = \left \{
\begin{array}{l}
S_{k,\Delta}^{(1)}(x) = a_0^{(1)} + a_1^{(1)}(x-x_0)+ \cdots + a_k^{(1)}(x-x_0)^k \\
\vdots \\
S_{k,\Delta}^{(i)}(x) = a_0^{(i)} + a_1^{(i)}(x-x_{i-1}) + \cdots + 
a_k^{(i)}(x-x_{i-1})^k \\
\vdots \\
S_{k,\Delta}^{(n)}(x) =  a_0^{(n)} + a_1^{(n)}(x-x_{n-1}) + \cdots 
+a_k^{(n)}(x-x_{n-1})^k.
\end{array}\right.
\]
\begin{notabene}
$S_{k,\Delta}^{(1)}(x)$ vale per gli $x \in [x_0, x_1]$, 
$S_{k,\Delta}^{(2)}(x)$  vale per gli $x \in [x_1, x_2]$, etc.
\end{notabene}
\end{defi}

\begin{flushleft}
Gradi di libertà: $(k+1)n$.\\
Vincoli:\\
\begin{tabular}{rl}
$k(n-1)$ &$+$\\
$ n-1$ &$+$\\
$ n+1 $&$=$\\
\hline 
$(k+1)n -(k-1)$
\end{tabular}
\end{flushleft}

Mancano $k-1$ vincoli, quindi abbiamo $\infty^{k-1}$ spline cubiche 
interpolanti possibili. Quindi avremo bisogno di aggiungere condizioni
pari al grado della spline meno uno ($k-1$).

\subsubsection{Momento della spline.}
Costruiamo una spline cubica interpolante con un \emph{metodo} che da come
vantaggio la possibilità di risolvere un sistema di ordine inferiore a
$4n$. Avremo una matrice \emph{tridiagonale}.
\begin{itemize}
\item[$\bullet$] Sostituiamo il problema del calcolo dei coefficienti con il
calcolo di parametri che permetteranno di ricostruirli.
\item[$\bullet$] Computazionalmente la spline cubica equivale ad una matrice
di $n$ righe e $4$ colonne.
\end{itemize}
I parametri che calcoliamo saranno il \emph{momento della spline}.
\begin{defi}(Momento della spline)\\
Il \emph{momento della spline} di ordine $i$ è definito come segue:
\[
M_i := \left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]''_{x = x_i} = 
\left[S_{3,\Delta}^{(i)}(x) \right]''_{x = x_i}.
\]
In altri termini è una riduzione delle incoglite del sistema lineare.
\end{defi}

Com'è, nel tratto $[x_{i-1}, x_i]$, la derivata seconda della spline? E'
un polinomio di primo grado. Se conoscessimo i due valori $M_i$ e $M_{i-1}$
avremmo la seguente forma:
\[
\left[S_{3,\Delta}^{(i)}(x) \right]'' = 
\frac{(x-x_{i-1})M_i + (x_i-x)M_{i-1}}{h_i}.
\]

Integrando $[S_{3,\Delta}^{(i)}(x)]''$ otteniamo:
\[
\left[S_{3,\Delta}^{(i)}(x) \right]' = \frac{(x-x_{i-1})^2}{2h_i}M_i 
- \frac{(x_i-x)^2}{2h_i}M_{i-1} + A_1.
\]

Integrando ulteriormente:
\[
S_{3,\Delta}^{(i)}(x) = \frac{(x-x_{i-1})^3}{6h_i}M_i 
+ \frac{(x_i-x)^3}{6h_i}M_{i-1} + A_1(x-x_{i-1}) + B_i.
\]

Come otteniamo $A_i$ e $B_i$? Imponiamo l'interpolazione.

\[
y_{i-1} = S_{3,\Delta}^{(i)}(x_{i-1}) = 
\frac{(x_i-x_{i-1})^{\cancelto{2}{3}}}{6\cancel{h_i}}M_{i-1} + B_i
= \frac{h_i^2 M_{i-1}}{6}+ B_i.
\]
\[ \longrightarrow \
B_i = y_{i-1} - \frac{h_i^2 M_{i-1}}{6}.
\]

\[
y_{i} = S_{3,\Delta}^{(i)}(x_{i}) = h_i^3
\frac{M_i}{6h_i} + A_ih_i+ B_i.
\]
\[
A_i = \frac{y_{i} -\frac{h_i^2M_i}{6}-B_i}{h_i}.
\]

\[
A_i = \frac{y_{i} -y_{i-1} + \frac{h_i^2 M_{i-1}}{6}-\frac{h_i^2M_i}{6}}{h_i}.
\]
\[\longrightarrow \
A_i = \frac{y_{i} -y_{i-1}}{h_i} + \frac{h_i}{6}\left(M_{i-1}-M_i
\right).
\]

\textbf{Problema $1$}: costruire il sistema che calcoli i momenti $M_i$.\\

\textbf{Problema $2$}: calcolare i coefficienti $a_{k}^{(i)}\quad k = 0,\ldots,
 4n$ nota questa forma alternativa (i momenti) del polinomio.

Iniziamo a risolvere il problema $2$.
\begin{itemize}
\item[$\bullet$]$a_0^{(i)} = S_{3,\Delta}^{(i)}(x_{i}) = y_{i}.$
\item[$\bullet$]$a_1^{(i)} = $?
\[a_1^{(i)} =
\left[S_{3,\Delta}^{(i)}(x) \right]'_{x = x_{i-1}} 
= \left[a_1^{(i)} + 2a_2^{(i)}(x-x_{i-1}) 
+ 3a_3^{(i)}(x-x_{i-1})^2\right]_{x = x_{i-1}}.
\]
\[a_1^{(i)} =
-\frac{h_i}{2}M_{i-1} + A_i.
\]
\item[$\bullet$]$a_2^{(i)} = $?
\[2a_2^{(i)} =
\left[S_{3,\Delta}^{(i)}(x) \right]''_{x = x_{i-1}} 
= \left[ 2a_2^{(i)} + 6 a_3^{(i)}(x-x_{i-1})\right]_{x = x_{i-1}}
\]
\[2a_2^{(i)} = M_{i-1}.
\]
\[a_2^{(i)} = \frac{M_{i-1}}{2}.\]
\item[$\bullet$]$a_3^{(i)} = $?
\[
6a_3^{(i)} =
\left[S_{3,\Delta}^{(i)}(x) \right]''_{x = x_{i-1}} = \frac{M_i - M_{i-1}}{h_i}.
\]

\[
a_3^{(i)} = \frac{M_i - M_{i-1}}{6h_i}.
\]
\end{itemize}

I nodi da calcolare sono $n+1$. Dobbiamo risolvere il problema $1$, ovvero
costruire un sistema che abbia come incognite i momenti $M_i, \quad i =0,
\ldots, n$ (gli $n+1$ nodi). Abbiamo già usato:
\begin{itemize}
\item[$\bullet$] l'interpolazione, ovvero abbiamo usato $\cc^0$ perchè
il momento sinistro è uguale al momento destro della spline.
\item[$\bullet$]$\cc^2$, sempre per l'uguaglianza tra momento sinistro e 
destro.  
\end{itemize}

Manca da utilizzare la continuità della derivata prima nei nodi interni, 
avendo $n-1$ vincoli restano sempre due gradi di libertà.

\[
\left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]'' = 
\left[S_{3,\Delta}^{(i)}(x) \right]''
= \frac{(x-x_{i-1})}{h_i}M_i + \frac{(x_i-x)}{h_i}M_{i-1}.
\]

\[
\left[S_{3,\Delta}^{(i)}(x) \right]' = \frac{(x-x_{i-1})^2}{2h_i}M_i 
- \frac{(x_i-x)^2}{2h_i}M_{i-1} +A_i.
\]
\[
\left[S_{3,\Delta}^{(i)}(x) \right]' = \frac{(x-x_{i-1})^2}{2h_i}M_i 
- \frac{(x_i-x)^2}{2h_i}M_{i-1} + \frac{y_{i} -y_{i-1}}{h_i} + 
\frac{h_i}{6}\left(M_{i-1}-M_i\right).
\]

\[
S_{3,\Delta}^{\phantom{(1)}}(x) = \frac{(x-x_{i-1})^3}{6h_i}M_i +
\frac{(x_i-x)^3}{6h_i}M_{i-1} + A_i(x-x_{i-1}) + B_i.
\]

Usiamo ora l'unica condizione che manca: la continuità della derivata prima
dei nodi interni $x_1, \ldots, x_{n-1}$.

\[
\underbrace{\left[S_{3,\Delta}^{(i)}(x) \right]'}_{\substack{x_i \leq x \leq  x_{i+1} \\ 
h_{i+1} = x_{i+1}-x_i}} = \frac{(x-x_{i})^2}{2h_{i+1}}M_{i+1} 
- \frac{(x_{i+1}-x)^2}{2h_{i+1}}M_{i} + \frac{y_{i+1} -y_{i}}{h_{i+1}} + 
\frac{h_{i+1}}{6}\left(M_{i}-M_{i+1}\right).
\]

Dobbiamo imporre la continuità della derivata prima nel nodo $x_i$, cioé:
\[
\left[S_{3,\Delta}^{(i)}(x) \right]'_{x = x_i} = \left[S_{3,\Delta}^{(i+1)}(x) 
\right]'_{x=x_i},
\]
in altri termini:
\[
\lim_{x \to x_i^-} \left[S_{3,\Delta}^{(i)}(x) \right]' =
\lim_{x \to x_i^+} \left[S_{3,\Delta}^{(i+1)}(x) \right]'.
\]
E' sufficiente calcolare i limiti nel punto:

\[
\frac{h_iM_i}{2} + \frac{h_{i}}{6}M_{i-1} - \frac{h_{i}}{6}M_{i} +
\frac{y_{i} -y_{i-1}}{h_i} = -
\frac{h_{i+1}M_i}{2} + \frac{h_{i+1}}{6}M_{i} - \frac{h_{i+1}}{6}M_{i+1} +
\frac{y_{i+1} -y_{i}}{h_{i+1}}.
\]
\[\Updownarrow\]
\[
\frac{h_{i}}{6}M_{i-1} + M_i\left[\frac{h_i}{2} - \frac{h_{i}}{6} +
\frac{h_{i+1}}{2}  - \frac{h_{i+1}}{6}\right] + \frac{h_{i+1}}{6}M_{i+1} =
\frac{y_{i+1} -y_{i}}{h_{i+1}} - \frac{y_{i} -y_{i-1}}{h_{i}}.
\]
\[\Updownarrow\]
\[
\frac{h_{i}}{6}M_{i-1} + \frac{2}{6}M_i(h_i+h_{i+1}) + \frac{h_{i+1}}{6}M_{i+1} =
\frac{y_{i+1} -y_{i}}{h_{i+1}} - \frac{y_{i} -y_{i-1}}{h_{i}}.
\]
\[\Updownarrow\]
\[
\frac{h_{i}}{h_{i}+h_{i+1}}M_{i-1} + 2M_i + \frac{h_{i+1}}{h_{i}+h_{i+1}}M_{i+1} =
\frac{6}{h_{i}+h_{i+1}}\left(\frac{y_{i+1} -y_{i}}{h_{i+1}} - 
\frac{y_{i} -y_{i-1}}{h_{i}}\right).
\]

Posti $\alpha_i$, $\beta_i$ e $d_i$ tali che:
\[
\alpha_i = \frac{h_{i}}{h_{i}+h_{i+1}}, \quad\beta_i =\frac{h_{i+1}}{h_i+h_{i+1}},
\quad d_i = \frac{6}{h_{i}+h_{i+1}}\left(\frac{y_{i+1} -y_{i}}{h_{i+1}} - 
\frac{y_{i} -y_{i-1}}{h_{i}}\right),
\]

possiamo esprimere il limite di cui sopra come segue:
\[\alpha_i M_{i-1} + 2M_i + \beta_iM_{i+1} = d_i.\]

\begin{itemize}
\item[] $i = 1$:\\
$\alpha_1 M_0 + 2M_1 + \beta_1M_2 = d_1.$
\item[] $i = 2$:\\
$\alpha_2 M_1 + 2M_2 + \beta_2M_3 = d_2.$
\item[]$\vdots$
\item[]$i = n-1$:\\
$\alpha_{n-1} M_{n-2} + 2M_{n-1} + \beta_{n-1}M_{n} = d_{n-1}.$
\end{itemize}

Risulta quindi un sistema di $n-1$ equazioni in $n+1$ incognite, $d_i$ è il
vettore dei termini noti, possiamo scrivere il sistema in forma matriciale
($A$):
\[A = \left(
\begin{array}{ccccccc}
\alpha_1 & 2 & \beta_1 & 0 & &\cdots & 0 \\
0 & \alpha_2 & 2 & \beta_2 &0&\cdots & \vdots \\
\vdots & & & & & & 0\\
0 & \cdots & &0 &\alpha_{n-1} & 2& \beta_{n-1}
\end{array}
\right), \quad d_i =
\left(\begin{array}{c}
d_1 \\
d_2 \\
\vdots \\
d_{n-1}
\end{array}
\right).\]
Occorre introdurre delle condizioni ulteriori poiché abbiamo un numero di
incognite maggiore del numero di equazioni. 

\subsection{Spline cubica naturale.}
Una soluzione è la spline cubica
naturale: $M_0 = M_n = 0$.
\[
\longrightarrow \ \left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]''_{x = x_0}
= \left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]''_{x = x_n} = 0.
\]
Ora abbiamo due incognite in meno, quelle date dai momenti $M_0$ e $M_n$.
Abbiamo ora un sistema in cui spariscono $\alpha_1$ e $\beta_{n-1}$.

\[\left\{
\begin{array}{l}
2M_1 + \beta_1M_2 = d_1 \\
\alpha_2M_1 + 2M_2 + \beta_2M_3 = d_2 \\
\vdots \\
\alpha_{n-1}M_{n-2}+ 2M_{n-1} = d_{n-1}
\end{array}
\right.\]

\[A = 
\left[
\begin{array}{ccccc}
 2 & \beta_1 & 0  &\cdots & 0 \\
\alpha_2 & 2 & \beta_2 & & \vdots \\
0 &\ddots &\ddots &\ddots &  0\\
\vdots & &\alpha_{n-2} & 2& \beta_{n-2}\\
0 & \cdots & 0 &\alpha_{n-1} & 2
\end{array}
\right].
\]

$A \in \rr^{(n-1)\times(n-1)}$ matrice tridiagonale irriducibile.\\

$\alpha_i + \beta_i = 1, \quad i = 2,\ldots n-1$.\\

$0 < \beta_1 \leq 1$.\\

$0 < \alpha_{n-1} \leq 1$.

\begin{osse}
La matrice $A$ è una matrice diagonal dominante:
\[
\left\{ \begin{array}{l}
\alpha_i + \beta_i = 1, \\
\beta_1 < 1 < 2,\\
\alpha_{n-1} < 1 < 2.
\end{array}\right.
\]
\end{osse}

Abbiamo quindi il seguente sistema lineare:
\[
Ax = d, \quad d = \left[\begin{array}{c}
d_1 \\
d_2 \\
\vdots \\
d_{n-1}
\end{array}
\right], \quad 
x = \left[\begin{array}{c}
M_1 \\
M_2 \\
\vdots \\
M_{n-1}
\end{array}
\right].
\]
La proprietà diagonal dominante è utile perchè permette di affermare che la
matrice non ha autovalore nullo. Ovvero è non singolare.

\subsection{Spline cubica a valori assegnati.}
Posti $M_0 = \overline{M}_0$ e $M_n = \overline{M}_n$ il nuovo vettore dei
termini noti $d$ risulta:
\[
d = \left[\begin{array}{l}
d_1 -  \alpha_1\overline{M}_0\\
d_2 \\
\vdots \\
d_{n-2}\\
d_{n-1} - \beta{n-1}\overline{M}_n
\end{array}
\right]
\]
La matrice $A$ invece non cambia.

\subsection{Spline cubica vincolata.}
\[
\left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]'_{x = x_0}= y'_0 
\quad \wedge \quad  \left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]'_{x = x_n} = y'_n.
\]

Sfruttando una logica diversa da prima
otteniamo due ulteriori vincoli (equazioni), la nostra matrice $A$ 
rappresenterà dunque un sistema lineare a $n+1$ equazioni in $n+1$ incognite.

\[
\underbrace{\left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]'}
_{ x_0 \leq x \leq x_1} = \left[S_{3,\Delta}^{(1)}(x) \right]'_{x = x_0} =
\]
\[=
\left[
\cancelto{0}{\frac{(x-x_{0})^2}{2h_1}M_1} 
- \frac{(x_1-x)^2}{2h_1}M_{0} + \frac{y_{1} -y_{0}}{h_1} + 
\frac{h_1}{6}\left(M_{0}-M_1\right) \right]_{x = x_0}
= y'_0 
\]
\[\Updownarrow\]
\[
- \frac{h_1}{2}M_0 + \frac{h_1}{6}M_0 - \frac{h_1}{6}M_1 
+ \frac{y_{1} -y_{0}}{h_1} = y'_0 
\]
\[\Updownarrow\]
\[
- M_0\left(\frac{h_1}{2}-\frac{h_1}{6} \right) - \frac{h_1}{6}M_1 
+ \frac{y_{1} -y_{0}}{h_1} = y'_0 
\]
\[\Updownarrow\]
\[
\frac{2}{6}h_1M_0 + \frac{h_1}{6}M_1  = \frac{y_{1} -y_{0}}{h_1} -y'_0 
\]
\[\Updownarrow\]
\[
2 M_0 + M_1 = 6y_1 -6y_0 - \frac{6 y'_0}{h_1}.
\]

Dunque, posto $d_0 = 6y_1 -6y_0 - \frac{6 y'_0}{h_1}$, abbiamo il seguente
risultato:
\[2 M_0 + M_1 = d_0.
\]

Analogamente, posto $d_n = 6y_{n} - 6y_{n-1} - \frac{6 y'_{n-1}}{h_n}$, 
otteniamo:
\[2 M_{n-1} + M_n = d_n.\]

Otteniamo quindi la seguente matrice $A$, tridiagonale, irriducibile,
diagonal dominante:
\[A =
\left[
\begin{array}{cccccc}
2 & 1 & 0 & \cdots & &0 \\
\alpha_1&  2 & \beta_1 & 0  &\cdots & 0 \\
0 &\alpha_2 & 2 & \beta_2 & & \vdots \\
0  & &\ddots &\ddots &\ddots &  0\\
\vdots & & &\alpha_{n-1} & 2& \beta_{n-1}\\
0 & & \cdots & 0 &1 & 2
\end{array}
\right],
\]
ed il seguente sistema lineare:
\[
Ax = d, \quad d = \left[\begin{array}{c}
d_0 \\
d_1 \\
\vdots \\
d_{n-1} \\
d_n
\end{array}
\right], \quad 
x = \left[\begin{array}{c}
M_0 \\
M_1 \\
\vdots \\
M_{n-1} \\
M_n
\end{array}
\right].
\]

Consideriamo $A$ sopra di ordine $n+1$, $x \in \rr^{n+1}, x \neq 0, y =Ax$.
Cosa si può dire di $\|x\|_{\infty}$ e di $\|y\|_{\infty}$?

\begin{prop}
Sia $r$ l'indice per cui:
\[|x_r| = \max_{0 \leq i \leq n}|x_i| = \|x\|_{\infty}.
\]
Siano $\alpha_0 = 0$, $\beta_r = 0$ (per operare sulla prima riga e 
sull'ultima colonna),
\[
\begin{array}{lcl}
|y_r| & = & |\alpha_r x_{r-1} + 2x_r + \beta_rx_{r+1}| \\
   & \geq &  2|x_r|- \alpha_r|x_{r-1}| -\beta_{r}|x_{r+1}| \\
   & \geq & 2|x_r|- \alpha_r|x_{r}|-\beta_{r}|x_{r}| \\
   & \geq & |x_r|
= \|x\|_{\infty}.
\end{array}
\]
Ovvero che $\|y\|_{\infty} \geq |y_r|$ in quanto è il $\max |y_i|$. 
\[
\|y\|_{\infty} \geq |y_r| \geq \|x\|_\infty \ \longrightarrow \ \|y\|_{\infty} 
\geq \|x\|_\infty.
\]  
Il vettore $x$ viene trasformato mediante $A$ in un vettore di norma $\infty$
maggiore. Con questo risultato dimostreremo la non singolarità di $A$.
\end{prop}
\begin{dimo}
Sia $A$ non singolare, allora $x \in \rr^{n+1}, \, x \neq 0, \, Ax = 0$.
\[
\longrightarrow \ \|Ax\| = \|0\| = 0 \geq \|x\|_\infty > 0. \quad 
\textrm{Assurdo.}
\]
\end{dimo}

\subsection{Spline cubica periodica.}

\[
\underbrace{\left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]'_{x = x_0}
=\left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]'_{x = x_n}}
_{\textrm{equazione aggiuntiva (*)}};
\ \underbrace{\left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]''_{x = x_0}
=\left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]''_{x = x_n}}_{M_0 = M_n}.
\]

Ponendo $M_0 = M_n$ riduciamo il sistema ad $n$ incognite, con l'equazione
aggiuntiva avremo un sistema $n \times n$.


\[
\underbrace{\left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]'}
_{ x_0 \leq x \leq x_1} = \left[S_{3,\Delta}^{(1)}(x) \right]'_{x = x_0} =
\]
\[=
\left[
\frac{(x-x_{0})^2}{2h_1}M_1 
- \frac{(x_1-x)^2}{2h_1}M_{0} + \frac{y_{1} -y_{0}}{h_1} + 
\frac{h_1}{6}\left(M_{0}-M_1\right) \right]_{x = x_0}.
\]

\[
\underbrace{\left[S_{3,\Delta}^{\phantom{(1)}}(x) \right]'}
_{ x_0 \leq x \leq x_1} = \left[S_{3,\Delta}^{(n)}(x) \right]'_{x = x_n} =
\]
\[=
\left[
\frac{(x-x_{n-1})^2}{2h_n}M_n 
- \frac{(x_n-x)^2}{2h_n}M_{n-1} + \frac{y_{n} -y_{n-1}}{h_n} + 
\frac{h_n}{6}\left(M_{n-1}-M_n\right) \right]_{x = x_n}.
\]

\[\Updownarrow\]
\[
\left[S_{3,\Delta}^{(1)}(x) \right]'_{x = x_0} =
-\frac{h_1}{2}M_0 + \frac{h_1}{6}M_0 - \frac{h_1}{6}M_1 
+ \frac{y_{1} -y_{0}}{h_1}.
\]

\[
\left[S_{3,\Delta}^{(n)}(x) \right]'_{x = x_n} =
\frac{h_n}{2}M_{n} + \frac{h_n}{6}M_{n-1} - \frac{h_n}{6}M_n 
+ \frac{y_{n} -y_{n-1}}{h_n}.
\]

Dalla relazione (*) otteniamo:
\[
-\frac{h_1}{2}M_0 + \frac{h_1}{6}M_0 - \frac{h_1}{6}M_1 
+ \frac{y_{1} -y_{0}}{h_1} =
\frac{h_n}{2}M_{n} + \frac{h_n}{6}M_{n-1} - \frac{h_n}{6}M_n 
+ \frac{y_{n} -y_{n-1}}{h_n},
\]
avendo imposto $M_0 = M_n$ otteniamo dunque:
\[
-\frac{h_n}{6}M_{n-1} + M_n\left(-\frac{h_1}{2}+ \frac{h_1}{6}+ \frac{h_n}{6}
-\frac{h_n}{2}\right) - \frac{h_1}{6}M_1 = - \frac{y_{1} -y_{0}}{h_1} +
\frac{y_{n} -y_{n-1}}{h_n}.
\]

\[\Updownarrow\]
\[
\frac{h_1}{6}M_1 + \frac{2}{6}M_n\left(h_1+h_n\right) + \frac{h_n}{6}M_{n-1}
= \frac{y_{1} -y_{0}}{h_1} - \frac{y_{n} -y_{n-1}}{h_n}.
\]

\[\Updownarrow\]
\begin{center}moltiplichiamo per $\frac{6}{h_1 + h_n}$
\end{center}
\[\Updownarrow\]
\[
\underbrace{\frac{h_1}{h_1 + h_n}}_{\alpha_1}M_1 + 2M_n + 
\underbrace{\frac{h_n}{h_1 + h_n}}_{\beta_1}M_{n-1} = 
\frac{6}{h_1 + h_n} \cdot \frac{y_{1} -y_{0}}{h_1} - \frac{y_{n} -y_{n-1}}{h_n}
\]

\begin{osse}
Si noti che con questa procedura perdiamo la struttura tridiagonale della 
matrice poiché l'equazione aggiunta è del tipo: $ \alpha_nM_1 + \cdots
+ \beta_n M_{n-1} + 2 M_n $ e il vincolo $M_0 = M_n$ trasforma la prima
equazione in: $2M_1 + \beta_1M_2 + \cdots + \alpha_1M_n$.

\[A =
\left[
\begin{array}{cccccc}
2 & \beta_1 & 0 & \cdots &0 &\alpha_1 \\
\alpha_2&  2 & \beta_2 & 0  &\cdots & 0 \\
0 &\alpha_3 & 2 & \beta_3 & & \vdots \\
0  & &\ddots &\ddots &\ddots &  0\\
\vdots & & &\alpha_{n-1} & 2& \beta_{n-1}\\
\alpha_n & & \cdots & 0 &\beta_n & 2
\end{array}
\right].
\]
\end{osse}

In questo caso occorre usare la tecnica gaussiana o tecniche analoghe, però
tale procedimento potrebbe ``sporcare'' gli zeri. La tecnica più adatta è di 
tipo iterativo ---non fatta in questo corso--- che sfrutta la forte
sparsità della matrice e non sporca gli zeri.

\section{Caratterizzazione delle spline cubiche.}

\begin{defi}
Sia $f \in \cc^2([a,b])$, si dice \emph{pseudo norma} di $f$ il numero reale
$\|f\|^2$ tale che:
\[
\|f\|^2 := \int_a^b\left[f''(x)\right]dx.
\]

Non è una norma perchè esiste una $f \neq 0$ tale che:
\[\int_a^b\left[f''(x)\right]dx = 0.\]
\end{defi}

Sia $\Delta = \{ a = x_0 < x_1 < \cdots < x_n = b\}$ una decomposizione
arbitraria di $[a,b]$, $S_{3,\Delta}(x)$ una spline cubica relativa alla
decomposizione ---non è richiesto che sia interpolante---.

\begin{teo}(Holiday)
Sia $f(x) \in \cc^0([a,b])$, $\Delta$ una decomposizione arbitraria di $[a,b]$
e $S_{3,\Delta}(x)$  una spline cubica relativa alla decomposizione allora:

\[
\|f(x) - S_{3,\Delta}(x)\|^2 = 
\]
\[
\begin{array}{cl}
= &\|f(x)\|^2 \\
  &- 2 \left[ \left.\left(f'(x)- S'_{3,\Delta}(x)\right) 
S''_{3,\Delta}(x)\right|_{x = a}^{x= b} + \left.\sum_{i=1}^n(f(x) - S_{3,\Delta}(x))
S''_{3,\Delta}(x)\right|_{x = x_{i-1}^+}^{x=x_i^-}\right] \\
& - \| S_{3,\Delta}(x)\|^2.
\end{array}
\]
\end{teo}

Sia ora $S_{3,\Delta}(x)$ una spline cubica relativa alla decomposizione
$\Delta$ \emph{interpolante} la funzione $f$.

\begin{itemize}
\item[$\bullet$]Per spline cubica interpolante naturale:
\[
\|f(x) - S_{3,\Delta}(x)\|^2 = \|f(x)\|^2 + 2[0] - \| S_{3,\Delta}(x)\|^2 > 0.
\]
\[\longrightarrow \|f(x)\|^2 \geq \| S_{3,\Delta}(x)\|^2.\]
$f, S \in \cc^2$, osserviamo che tra tutte le funzioni $\cc^2$ che passano per
quei punti, la spline cubica interpolante naturale è quella che minimizza
il funzionale $\|\cdot\|^2.$

Analogamente per le spline cubiche vincolate con $f$ che soddisfa le relazioni
di derivata prima agli estremi.
\item[$\bullet$]Per la spline cubica interpolante vincolata:
\[
\left.S'_{3,\Delta}(x)\right|_{x = x_{0}} = f'(x_0), \quad
\left.S'_{3,\Delta}(x)\right|_{x = x_{n}} = f'(x_n).
\]
Andando a calcolare la pseudo norma la quantità $\sum_{i=1}^n(f(x) - 
S_{3,\Delta}(x))$ si annulla poiché $S$ è interpolante, la quantità 
$f'(x)- \left.S'_{3,\Delta}(x)\right|_{x = a}^{x= b}$ si annulla perchè è
differenza.
\[\longrightarrow \|f(x)\|^2 \geq \| S_{3,\Delta}(x)\|^2.\]
La funzione spline vincolata minimizza $\|\cdot\|^2$ nell'insieme delle 
funzioni $f \in \cc^2$ che passano per quei punti e vincolate ad avere tali
valori $f'(x_0)$ e $f'(x_n)$.
\item[$\bullet$]Per spline cubica interpolante periodica:
\[
\|f(x)\|^2 \geq \| S_{3,\Delta}(x)\|^2.
\]
Minimizzando $\|\cdot\|^2$ vuol dire che $S_{3,\Delta}(x)$ minimizza la 
curvatura nell'intervallo $[a,b]$ (come somme di curve locali).
\end{itemize} 
 
Essendo $\|\cdot\|^2$ un funzionale di energia, la spline minimizza l'energia?

\begin{defi}
Sia $y = g(x)$, $x \in [a,b]$, la curvatura $c$ in $x$ di $g(x)$ è definita 
come segue:
\[
c(x) := \frac{g''(x)}{[1 + (g'(x))^2]^{\frac{3}{2}}}.
\]
Se $g'(x)$ è prossimo a $0$ il denominatore è prossimo a $1$.
\end{defi}

La spline è quella funzione che passa nei punti segnati e che minimizza
la curvatura sull'intervallo.

\begin{equation}\label{eq8.1}
\|f(x) - S_{3,\Delta}(x)\|^2 = \|f(x)\|^2 -\|S_{3,\Delta}(x)\|^2.
\end{equation}
Ultilizzeremo l'equazione \ref{eq8.1} per dimostrare l'unicità di una
spline cubica naturale, vincolante o periodica.

\section{Unicità delle spline cubiche.}
\begin{prop}
Siano $S_{3,\Delta}(x) \in \cc^2$ e $\overline{S}_{3,\Delta}(x) \in \cc^2$, 
due spline cubiche interpolanti la funzione $f$ nei medesimi punti. Allora:
\[
\overline{S}_{3,\Delta}(x) = S_{3,\Delta}(x).
\]
\end{prop}
\begin{dimo}
Supponiamo  esistano $S_{3,\Delta}(x)$ e $\overline{S}_{3,\Delta}(x)$, due spline
cubiche interpolanti soddisfacenti le medesime condizioni.

Se $\overline{S}_{3,\Delta}(x)$ ha le stesse caratteristiche di $S$, cioé
interpola nei medesimi punti $f$ e appartiene alla classe $\cc^2$ allora la 
possiamo ``prendere'' come $f$:
\begin{equation}\label{eq8.2}
\|\overline{S}_{3,\Delta}(x) - S_{3,\Delta}(x)\|^2
=\| \overline{S}_{3,\Delta}(x)\|^2 - \|S_{3,\Delta}(x)\|^2 \geq 0.
\end{equation}
Analogamente possiamo scambiare i ruoli di $\overline{S}_{3,\Delta}(x)$ e 
$S_{3,\Delta}(x)$:
\begin{equation}\label{eq8.3}
\|S_{3,\Delta}(x) - \overline{S}_{3,\Delta}(x)\|^2
=\|S_{3,\Delta}(x)\|^2- \|\overline{S}_{3,\Delta}(x)\|^2   \geq 0.
\end{equation}

Da \ref{eq8.2} e \ref{eq8.3} otteniamo:
\[
0 = \|S_{3,\Delta}(x)\|^2- \|\overline{S}_{3,\Delta}(x)\|^2 = 
\int_a^b[S''_{3,\Delta}(x)- \overline{S}''_{3,\Delta}(x)]^2dx.
\]

Siccome la funzione integranda è sempre positiva (il quadrato) l'unica
possibilità poiché l'integrale sia nullo è che la funzione integranda sia
identicamente nulla, cioè:
\[
\overline{S}''_{3,\Delta}(x) - S''_{3,\Delta}(x) = 0 \ \longrightarrow \
\overline{S}''_{3,\Delta}(x) = S''_{3,\Delta}(x).
\]
\[
\longrightarrow \ \overline{S}_{3,\Delta}(x) = S_{3,\Delta}(x) + cx +d.
\]
Deve succedere che:
\[
\left\{\begin{array}{r}ca+d = 0 \\ cb+d = 0\end{array} \right.
\quad \left[\begin{array}{cc}a & 1 \\ b&1\end{array} \right] = a-b \neq 0. 
\]
L'unica soluzione è quella nulla $(0,0)$ ovvero $c = d = 0$.
\[\longrightarrow \ \overline{S}_{3,\Delta}(x) = S_{3,\Delta}(x).\]
\end{dimo}


\section{Spline lineari.}
Ricordiamo il \emph{polinomio interpolatore di Lagrange}, costruito come
combinazione lineare dei polinomi \emph{lagrangiani} con coefficienti i valori
che deve interpolare (ovvero i dati):
\[
p(x) = \sum_{i=1}^n y_il_i(x).
\]
Possiamo recuperare questa idea nell'ambiente delle spline?

\subsection{Funzioni polinomiali a cappuccio (Spline a cappuccio).}
Costruiamo i seguenti polinomi lineari a cappuccio definiti a tratti:
\[
B_i(x) = \left\{
\begin{array}{llr}
\frac{x -x_{i-1}}{x_i- x_{i-1}} & &x_{i-1} \leq x \leq x_i, \\
\\
\frac{x -x_{i}}{x_{i+1} -x_{i}} & &x_{i} \leq x \leq x_{i+1}, \\
\\
0 & &\textrm{altrimenti.}
\end{array}
\right.
\]
Possiamo costruirli su un dominio costituito interamente dal nostro intervallo
(la decomposizione $\Delta$), per $i = 0, \ldots, n-1$. Cioè costruiamo un
polinomio lineare con i polinomi lineari ---e non con $1, \ldots, n$--- 
usando i due tratti delle $B_i$ e $B_{i+1}$ tra $[x_i,x_{i+1}]$. Ovvero:
\[
p(x) = \alpha_iB_i(x) + \alpha_{i+1}B_{i+1}(x), \quad x \in [x_i, x_{i+1}].
\]
Questo si può fare in ogni tratto tranne che in $[x_0, x_1]$ e in 
$[x_{n-1}, x_n]$, poiché il polinomio relativo in questi tratti non sarebbe 
completo. Ad esempio nel tratto $[x_0, x_1]$ avremmo solo la componente
$\alpha_1B_1(x)$ del polinomio, quindi o costruiamo un mezzo cappuccio oppure
aggiungiamo un punto ($x_{-1}$ in questo caso).

\[
B_0(x) = \left\{
\begin{array}{llr}
\frac{x_1 -x}{x_1 -x_0} & &x_0 \leq x \leq x_1, \\
\\
0 & &\textrm{altrimenti.}
\end{array}
\right.\]

\[
B_n(x) = \left\{
\begin{array}{llr}
\frac{x_n -x}{x_n -x_{n-1}} & &x_{n-1} \leq x \leq x_n, \\
\\
0 & &\textrm{altrimenti.}
\end{array}
\right.\]

\begin{prop}I polinomi
$B_i(x)$ definiti come sopra,
 con $i = 0, \ldots, n$ sono linearmente indipendenti.
\end{prop}
\begin{dimo}(traccia)
\[
\sum_{i=1}^n c_iB_i(x) = 0 \ \stackrel{\textrm{Ts}}{\Longleftrightarrow} \ 
c_i = 0\ \forall i = 1, \ldots, n. 
\]
Per concludere la dimostrazione è sufficiente prendere i nodi e valutare la
combinazione lineare, per ogni nodo si salva solo $B_i$ allora se $c_iB_i = 0$
si ha che $c_i = 0$.
\end{dimo}

Ritornando alle spline, definiamo la seguente spline lineare:
\[
S_{1,\Delta}(x) = \sum_{i=0}^n \alpha_iB_i(x).
\]
$S_{1,\Delta}(x)$ è interpolante, $S_{1,\Delta}(x_k) = y_k$ per ogni $k=0,\ldots,
n$.
\[
S_{1,\Delta}(x_k) = \sum_{i=0}^n \alpha_iB_i(x_k)=
\alpha_0\cancelto{0}{B_0(x_k)}+ \cancelto{0}{\cdots} + \alpha_kB_k(x_k) +
\cancelto{0}{\cdots} + \alpha_n\cancelto{0}{B_n(x_k)} = y_k.
\]
\[
\longrightarrow \ y_k = \alpha_k\cancelto{1}{B_k(x_k)} = \alpha_k.
\]
\[
\longrightarrow \ S_{1,\Delta}(x) = \sum_{i=0}^n y_iB_i(x).
\]
Abbiamo quindi ottenuto una spline lineare come combinazione lineare i cui
coefficienti sono i dati che già avevamo.
\[
B_i\ \textrm{interpola:} \quad\begin{array}{ccr}
x_0 & 0 & (x_0,0) \\
x_1 & 0 & (x_1,0) \\
\vdots \\
x_{i-1} & 0 & (x_{i-1},0) \\
x_i & 1 & (x_i,1)\\
x_{i+1} & 0 & (x_{i+1},0) \\
\vdots \\
x_n & 0 & (x_n,0) 
\end{array}
\]
\[
B_i(x_j) = \left\{ \begin{array}{lr}1 & \textrm{se} \ j=i \\ 0 &\textrm{se} 
\ j \neq i.\end{array}\right.
\]

\begin{osse}\label{oss8.17}
$B_i$ risente dell'errore solo nel tratto $[x_{i-1},x_{i+1}]$, mentre il
polinomio lagrangiano $l_i$ lo distribuisce su tutto $[a,b]$, abbiamo il 
grande vantaggio che tale errore, dove c'è, resti concentrato localmente.
\end{osse}

Se $f(x_i) = y_i$ allora:
\[S_{1,\Delta}(x) = \sum_{i=0}^n f(x_i)B_i(x).\]

\subsubsection{Proprietà: supporto.}
La proprietà che differenzia i polinomi lagrangiani dai polinomi a cappuccio
$B_i$ è quella data dall'osservazione \ref{oss8.17}, tale proprietà è
chiamata \emph{supporto}. 

\subsubsection{Proprietà: convergenza.}
Le spline lineari convergono alle funzioni di classe $\cc^2$. Invece non 
c'è convergenza con polinomi classici, neanche con i lagrangiani (es: Runghe).
 
