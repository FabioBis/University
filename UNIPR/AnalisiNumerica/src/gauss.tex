\hyphenation{
Barrow
Cramer
Gauss
Hausdorff
Laplace
Runghe
tras-for-ma-zio-ne
Torricelli
}

%Algoritmo di Gauss

\chapter{Algoritmo di Gauss.}
Lo scopo dell'algoritmo di Gauss è quello di trasformare una matrice quadrata
$A$ in una equivalente $A_n$ triangolare, nel nostro caso superiore.

\[
A \equiv A_1, \qquad
A = \left[
\begin{array}{cccc}
a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
a_{2,1} & a_{2,2}& \cdots &  a_{2,n} \\
\vdots & & \ddots & \vdots \\
a_{n,1}& a_{n,2} & \cdots & a_{n,n}
\end{array} \right].
\]
\section{Primo passo dell'algoritmo.}
Chiamiamo $a_{i,j}^{(1)} = a_{i,j}$. Poniamo quindi $A_1$ come segue:
\[
A_1 = \left[
\begin{array}{cccc}
a_{1,1}^{(1)} & a_{1,2}^{(1)} & \cdots & a_{1,n}^{(1)} \\
a_{2,1}^{(1)} & a_{2,2}^{(1)}& \cdots & _{2,n}^{(1)} \\
\vdots & &\ddots & \vdots \\
a_{n,1}^{(1)}& a_{n,2}^{(1)} & \cdots & a_{n,n}^{(1)}
\end{array} \right].
\]

\[
b_1 \equiv b, \qquad
b_1 = \left[ \begin{array}{c}
b_{1}^{(1)} \\
\vdots \\
b_{n}^{(1)}
\end{array}\right].
\]

Il primo passo consiste nell'eliminare (annullando) gli elementi
della prima colonna al di sotto del primo elemento $a_{1,1}^{(1)}$. Ecco il
nostro sistema nella forma originale, a cui abbiamo solo applicato le
etichette.
\[
\left\{
\begin{array}{cccc}
a_{1,1}^{(1)}x_1 & a_{1,2}^{(1)}x_2 & \cdots & a_{1,n}^{(1)}x_n = b_{1}^{(1)}\\
a_{2,1}^{(1)}x_1 & a_{2,2}^{(1)}x_2 & \cdots & a_{2,n}^{(1)}x_n = b_{2}^{(1)}\\
\vdots & &\ddots & \vdots \\
a_{n,1}^{(1)}x_1 & a_{n,2}^{(1)}x_2 & \cdots & a_{n,n}^{(1)}x_n = b_{n}^{(1)}
\end{array}\right .
\]
Ipotesi: $a_{1,1}^{(1)} \neq 0$. Modifichiamo la seconda riga come segue:
\[
r_1^{(1)}m + r_2^{(1)} \leadsto r_2^{(2)}.
\]
Le $r_i^{(k)}$ rappresentano le righe della nostra matrice nella posizione $i$,
$k$ è il numero della trasformazione\footnote{Se una riga o un coefficiente
ha apice $(k)$ significa che ha subito $k-1$ trasformazioni.}, occorre
calcolare le
combinazioni lineari che consentono di annullare i coefficienti desiderati.
Ovvero si deve trovare il valore di $m$ adatto.

\[
a_{1,1}^{(1)}m + a_{2,1}^{(1)} = 0, \qquad m = -\frac{a_{2,1}^{(1)}}{a_{1,1}^{(1)}},
\]
Se, tramite operazioni elementari sulle righe, si moltiplica per
questo $m$ la prima riga e si somma il risultato alla seconda, si ottiene il
coefficiente di $x_1$ uguale a $0$.

\[
a_{2,j}^{(2)} = a_{1,j}^{(1)}m + a_{2,j}^{(1)} \quad j = 2, \ldots, n.
\]

\[
b_2^{(2)} = b_1^{(1)}m + b_2^{(1)}
\]
La seconda riga verrà così trasformata nella seguente:
\[\left[
\begin{array}{ccccc}
0 & a_{2,2}^{(2)}x_2 & a_{2,3}^{(2)}x_3 & \cdots & a_{2,n}^{(2)}x_n = b_2^{(2)}
\end{array}\right].
\]

Si itera il processo su tutte le rimanenti righe della matrice, vediamo
il calcolo della terza riga: combinazione lineare tra la prima e la terza
riga (con un nuovo coefficiente $m$).
\[
r_1^{(1)}m + r_3^{(1)} \leadsto r_3^{(2)}.
\]
\[
a_{1,1}^{(1)}m + a_{3,1}^{(1)} = 0, \qquad m = -\frac{a_{3,1}^{(1)}}{a_{1,1}^{(1)}},
\]
\[
a_{3,j}^{(2)} = a_{1,j}^{(1)}m + a_{3,j}^{(1)} \quad j = 2, \ldots, n.
\]

\[
b_3^{(2)} = b_1^{(1)}m + b_3^{(1)}.
\]
Ottenendo così la terza riga:
\[ \left[
\begin{array}{ccccc}
0 & a_{3,2}^{(2)}x_2 & a_{3,3}^{(2)}x_3 & \cdots & a_{3,n}^{(2)}x_n = b_3^{(2)}
\end{array}
\right].
\]
Iterando quindi il procedimento alle rimanenti righe si ottiene dunque la
matrice equivalente $A_2$ come segue:
\[
A_2 = \left[
\begin{array}{cccc}
a_{1,1}^{(1)} & a_{1,2}^{(1)} & \cdots & a_{1,n}^{(1)} \\
0 & a_{2,2}^{(2)} & \cdots & a_{2,n}^{(2)} \\
\vdots & \vdots & \ddots & \vdots \\
0 & a_{n,2}^{(2)} & \cdots & a_{n,n}^{(2)}
\end{array} \right],
\qquad
b_2 = \left[ \begin{array}{c}
b_{1}^{(1)} \\
b_2^{(2)} \\
b_3^{(2)} \\
\vdots \\
b_{n}^{(2)}
\end{array}\right].
\]
All'inizio si è ipotizzato che $a_{1,1}^{(1)}$ fosse diverso da $0$, ma se
ciò non fosse non sarebbe un problema. E' infatti sufficiente prendere la riga
che abbia come primo elemento un coefficiente non nullo e scambiarla con la
prima. Sappiamo che questa esiste poiché la matrice di partenza $A$ è non
singolare.

\begin{figure}
Hp: $a_{1,1}^{(1)} \neq 0$.
\[
m_{i,1} = -\frac{a_{i,1}^{(1)}}{a_{1,1}^{(1)}} \quad i = 2, \ldots, n.
\]
\[
a_{i,j}^{(2)} = a_{1,j}^{(1)}m_{i,1} + a_{i,j}^{(1)} \quad j = 2, \ldots, n.
\]

\[
b_i^{(2)} = b_1^{(1)}m_{i,1} + b_i^{(1)}.
\]
\caption{Forma compatta.}
\end{figure}


Quanto costa questo algoritmo?
\begin{itemize}
\item[]Divisioni: $n-1$.
\item[]Prodotti\footnoteremember{costoattuale}{$(n-1)^2$ per la matrice e $(n-1)$ per il vettore dei termini noti.}: $(n-1)^2 + (n-1)$.
\item[]Somme\footnoterecall{costoattuale}: $(n-1)^2 + (n-1)$.
\end{itemize}\
Il costo è all'incirca $\textrm{O}(n^2)$, ovvero già a questo solo passaggio
costa di più della risoluzione di una matrice già in forma
triangolare.

\section{Passi successivi dell'algoritmo.}
In questo momento il sistema è nella seguente forma:
\[
\left\{
\begin{array}{cccc}
a_{1,1}^{(1)}x_1 & a_{1,2}^{(1)}x_2 & \cdots & a_{1,n}^{(1)}x_n = b_{1}^{(1)}\\
0 & a_{2,2}^{(2)}x_2 & \cdots & a_{2,n}^{(2)}x_n = b_{2}^{(2)}\\
\vdots & \vdots&\ddots & \vdots \\
0 & a_{n,2}^{(2)}x_2 & \cdots & a_{n,n}^{(2)}x_n = b_{n}^{(2)}
\end{array}\right .
\]
Ora occorre eliminare i coefficienti di $x_2$ al di sotto di $a_{2,2}^{(2)}$.
Possiamo ancora assumere l'ipotesi: $a_{2,2}^{(2)} \neq 0$ e iterare il
procedimento precedente.
\begin{figure}[h]
Hp: $a_{2,2}^{(2)} \neq 0$.
\[
m_{i,2} = -\frac{a_{i,2}^{(2)}}{a_{2,2}^{(2)}} \quad i = 3, \ldots, n.
\]
\[
a_{i,j}^{(3)} = a_{2,j}^{(2)}m_{i,2} + a_{i,j}^{(2)} \quad j = 3, \ldots, n.
\]

\[
b_i^{(3)} = b_2^{(2)}m_{i,2} + b_i^{(2)}.
\]
\caption{Forma compatta del secondo passo.}
\end{figure}
\[
[A_3|b_3] = \left[
\begin{array}{ccccc|c}
a_{1,1}^{(1)} & a_{1,2}^{(1)} & a_{1,3}^{(1)} & \cdots& a_{1,n}^{(1)} &  b_1^{(1)}\\
0 & a_{2,2}^{(2)} & a_{2,3}^{(2)} & \cdots& a_{2,n}^{(2)} &  b_2^{(2)}\\
0 & 0 & a_{3,3}^{(3)} & \cdots & a_{3,n}^{(3)} &b_3^{(3)}\\
\vdots & \vdots & \vdots & \ddots & \vdots&  \vdots\\
0 & 0 & a_{n,3}^{(3)} & \cdots & a_{n,n}^{(3)} & b_n^{(3)}
\end{array} \right].
\]

\begin{osse}\label{oss2.1}
Abbiamo potuto supporre come prima $a_{2,2}^{(2)} \neq 0$ poiché, almeno un
elemento
della seconda colonna è non nullo. Si noti che scambiando due righe la
soluzione del sistema non cambia, invece lo scambio tra colonne cambia
l'ordine delle soluzioni.
\end{osse}

Quanto costa questo passo?
\begin{itemize}
\item[]Divisioni: $n-2$.
\item[]Prodotti\footnoteremember{costoattuale2}{$(n-2)^2$ per la matrice e $(n-2)$ per il vettore dei termini noti.}: $(n-2)^2 + (n-2)$.
\item[]Somme\footnoterecall{costoattuale2}: $(n-2)^2 + (n-2)$.
\end{itemize}

\subsection{Passo $k$-esimo.}
Vediamo la generalizzazione dell'algoritmo al passo $k-1$. Avremo la matrice
$[A_k|b_k]$ così formata:
\[
[A_k|b_k] = \left[
\begin{array}{cccccc|c}
a_{1,1}^{(1)}&a_{1,2}^{(1)}& \cdots &a_{1,k}^{(1)}&\cdots& a_{1,n}^{(1)}&b_1^{(1)}\\
0 & a_{2,2}^{(2)} & \cdots & a_{2,k}^{(2)} & \cdots & a_{2,n}^{(2)} &  b_2^{(2)}\\
0 &0  & \ddots & \vdots &  & \vdots & \vdots\\
0 &0  &0 &a_{k,k}^{(k)} & \cdots & a_{k,n}^{(k)}&  b_k^{(k)}\\
0 &0  &0 &a_{k+1,k}^{(k)} & \cdots & a_{k+1,n}^{(k)}&  b_{k+1}^{(k)}\\
0 &0  &0 &\vdots &  & \vdots&  \vdots\\
0 &0  &0 &a_{n,k}^{(k)} & \cdots & a_{n,n}^{(k)} & b_n^{(k)}
\end{array} \right].
\]
\begin{figure}[h]
Hp: $a_{k,k}^{(k)} \neq 0$.
\[
m_{i,k} = -\frac{a_{i,k}^{(k)}}{a_{k,k}^{(k)}} \quad i = k+1, \ldots, n.
\]
\[
a_{i,j}^{(k+1)} = a_{k,j}^{(k)}m_{i,k} + a_{i,j}^{(k)} \quad j = k+1, \ldots, n.
\]

\[
b_i^{(k+1)} = b_k^{(k)}m_{i,k} + b_i^{(k)}.
\]
\caption{Forma compatta del passo $k-1$.}
\end{figure}

Costo del passo $k+1$.
\begin{itemize}
\item[]Divisioni: $n-k$.
\item[]Prodotti\footnoteremember{costoattualek}{$(n-k)^2$ per la matrice e $(n-k)$ per il vettore dei termini noti.}: $(n-k)^2 + (n-k)$.
\item[]Somme\footnoterecall{costoattualek}: $(n-k)^2 + (n-k)$.
\end{itemize}

\subsection{Termine dell'algoritmo.}
Dopo $n-1$ passi otteniamo la matrice $[A_n|b_n]$ così formata:
\[
[A_n|b_n] = \left[
\begin{array}{cccc c|c}
a_{1,1}^{(1)}&a_{1,2}^{(1)}& \cdots &\cdots& a_{1,n}^{(1)}&b_1^{(1)}\\
 & a_{2,2}^{(2)} & \cdots& \cdots & a_{2,n}^{(2)} &  b_2^{(2)}\\
 & &a_{3,3}^{(3)} & \cdots & a_{3,n}^{(3)}&  b_3^{(3)}\\
 & & &\ddots & \vdots & \vdots  \\
 & & & & a_{n,n}^{(n)} & b_n^{(n)}
\end{array} \right].
\]

Il vettore $x$ si calcola dunque risolvendo il problema:
\[
A_nx = b_n.
\]

\section{Costo totale dell'algoritmo.}
\begin{itemize}
\item[]Divisioni: \[(n-1) + (n-2) + (n-3) + \cdots + 1 = \sum_{i=1}^{n-1}i.\]
\item[]Prodotti: \[(n-1)^2 + (n-1) + (n-2)^2 + (n-2) + \cdots + 1^2 + 1 =
\sum_{i=1}^{n-1}i^2 + \sum_{i=1}^{n-1}i.\]
\item[]Somme: \[(n-1)^2 + (n-1) + (n-2)^2 + (n-2) + \cdots + 1^2 + 1 =
\sum_{i=1}^{n-1}i^2 + \sum_{i=1}^{n-1}i.\]
\end{itemize}
\[\sum_{i=1}^{n-1}i^2 + \sum_{i=1}^{n-1}i
= \frac{(n-1)^3}{3} + \frac{(n-1)^2}{2} + \frac{(n-1)}{6} + \frac{n(n-1)}{6}
\simeq \textrm{O}(\frac{n^3}{3}).
\]
Costo di prodotti e somme: $\textrm{O}(\frac{2}{3}n^3)$.
\begin{itemize}
\item[]$n^3$ è il costo di trasformazione della matrice $A$ di ordine $n$.
\item[]$n^2$ è il costo della soluzione di una matrice triangolare di ordine
$n$.
\end{itemize}
$\textrm{O}(n^3) + \textrm{O}(n^2) = \textrm{O}(n^3)$.

\begin{osse}
\label{gauss-determinante}
Questo algoritmo, se il determinante di $A$ è diverso da zero, permette anche
di calcolare il determinante stesso, a meno del segno:
\[ \textrm{det}A = \prod_{k=1}^na_{k,k}^{(k)}.
\]
Con Laplace il costo computazionale sarebbe dell'ordine di $n!$.
\end{osse}
\begin{osse}
\label{Gauss-inline}
Dobbiamo risolvere due sistemi lineari: $Ax = b$ e $Ax = c$, con $A \in
\rr^{n\times n}$ non singolare. Possiamo risolvere il primo sistema e poi il
secondo, ma è evidente che eseguiremmo le stesse operazioni per entrambi
i sistemi.

Soluzione:
\[[A_1|b_1|c_1] \leadsto [A_n|b_n|c_n].\]
Utilizzando l'algoritmo di eliminazione di Gauss sulla matrice di $n$ righe e
$n+2$ colonne di cui sopra otteniamo i due sistemi $A_nx = b_n$ e
$A_nx = c_n$.
\end{osse}
\begin{osse}
Come calcolare la matrice inversa?
\[A A^{-1} = A^{-1} A = I.\]

Sia $\overline{a}_i$ la colonna della matrice di indice $i$ di
$A^{-1}. \qquad (i = 1, 2, \ldots , n)$

\[
A[\overline{a}_1\, \overline{a}_2\, \ldots\, \overline{a_{i}}\, \ldots\,
\overline{a}_n] = [e_1\, e_2\, \ldots\, e_i\, \ldots\, e_n] = I.\]

\[[A\overline{a}_1\, A\overline{a}_2\, \ldots\, A\overline{a}_i\, \ldots
\,
A\overline{a}_n] = [e_1\, e_2\, \ldots\, e_i\, \ldots\, e_n] = I.\]

\[
\begin{array}{ccc}\begin{array}{ccc}
A\overline{a}_1& = & e_1 \\
A\overline{a}_2 & = & e_2 \\
\vdots & \vdots & \vdots \\
A\overline{a}_n & = & e_n
\end{array} & & [A|e_1\, e_2\, \ldots\, e_n] = [A|I].
 \end{array}
\]

$[A|I]$ risulta una matrice di $n$ righe per $2n$ colonne.
Applicando la trasformazione con l'algoritmo di Gauss otteniamo:
\[
[A|I] \leadsto [A_n|\tilde{I}] = [A_n | \tilde{e}_1\, \tilde{e}_2\, \ldots\,
\tilde{e}_n\,].
\]
\[
A_n\overline{a}_i = \tilde{e}_i \qquad i = 1, \ldots, n.
\]
Ciascuna risoluzione del sistema lineare ha un costo di $n^2$. Per la
trasformazione completa il costo non è più O$(\frac{n^3}{3})$. Occorre
trasformare $n$ vettori pari a $n^2$ in termini di costo.
\end{osse}

