\hyphenation{
Barrow
Binet
Cramer
Gauss
Hausdorff
Laplace
Runghe
tras-for-ma-zio-ne
Torricelli
}

\section{Matrici Tridiagonali}

Vediamo ora qualche risultato analogo utilizzando matrici tridiagonali.

\begin{defi}Sia $A \in \rr^{n \times n}$ una matrice tridiagonale:

\[ A =
\left[ \begin{array}{ccccc}
a_1    & c_1    & 0      & \cdots & 0 \\
b_2    & a_2    & c_2    & \ddots & \vdots \\
0      & b_3    & \ddots & \ddots & 0 \\
\vdots & \ddots & \ddots & a_{n-1}& c_{n-1} \\
0      & \cdots & 0      & b_{n} & a_n
\end{array} \right].
\]

Si definiscono:\\
$d_0 :=  1$;   \\
$d_1 :=  a_1$; \\
$\vdots$      \\
$d_k :=  a_k d_{k-1} - b_kc_{k-1}d_{k-2}. \qquad (k = 2, \ldots, n)$\\
\end{defi}

\begin{prop}Sia $A \in \rr^{n \times n}$ una matrice tridiagonale
definita come sopra, allora:
\[(\forall k) (1\leq k \leq n) \Rightarrow
(d_k = \textrm{det}\Delta_k \, \wedge \, \textrm{det}A = d_n).\]
\end{prop}
\begin{dimo}$d_n \stackrel{?}{=}\textrm{det}A.$

Si dimostra per induzione su $n$.
\begin{itemize}
\item[]Caso base: $n = 1$.
\begin{flushleft}
$d_1 = a_1 = \textrm{det}\Delta_1.$
\end{flushleft}
\item[]Ipotesi induttiva: $\Delta_{n-1} = d_{n-1}$.
\begin{flushleft}
Tesi: det$A = d_n$.
\end{flushleft}

\[
\begin{array}{lcl}
\textrm{det}A & = & a_n\textrm{det}\Delta_{n-1} + b_n(-1)\textrm{det}[*]\\
              & \stackrel{hp. ind}{=} & a_nd_{n-1} - b_nc_{n-1}d_{n-1} \\
              & = & d_n.
\end{array}
\]
Su $A$ abbiamo applicato lo sviluppo di Laplace, $[*]$ è la matrice su cui
abbiamo applicato lo sviluppo sull'ultima colonna.
\end{itemize}
\end{dimo}

\begin{osse}
Tale proposizione, se vista come algoritmo di risoluzione del determinante
ha costo pari a $O(3(n-1))$. Ovvero ha costo lineare (strettamente minore
a $n^3$\footnote{Sarebbe il costo del calcolo del determinante in caso si sia
fatto uso dell'algoritmo di Gauss. Si veda l'osservazione \cite{gauss-determinante}}).
La struttura della matrice quindi, cambia pesantemente il costo computazionale.
\end{osse}

Come si calcola il polinomio caratteristico di una matrice tridiagonale?

\[ A - \lambda I =
\left[ \begin{array}{ccccc}
a_1- \lambda    & c_1    & 0      & \cdots & 0 \\
b_2    & a_2 - \lambda   & c_2    & \ddots & \vdots \\
0      & b_3    & \ddots & \ddots & 0 \\
\vdots & \ddots & \ddots & a_{n-1}- \lambda& c_{n-1} \\
0      & \cdots & 0      & b_{n} & a_n - \lambda
\end{array} \right].
\]
\[
\textrm{det}(A-\lambda I)?\]
$d_0(\lambda) :=  1$;   \\
$d_1(\lambda) :=  a_1 - \lambda$; \\
$\vdots$      \\
$d_k(\lambda) :=  (a_k- \lambda) (d_{k-1}(\lambda) - b_kc_{k-1}d_{k-2}(\lambda)).
 \qquad (k = 2, \ldots, n)$\\

\begin{flushleft}
$d_n(\lambda)$ è il polinomio caratteristico di $A$.
\end{flushleft}

\begin{osse}
Non si calcola $d_n(\lambda)$ (nella sua espressione generica), serve solo
il valore di $d_n(\lambda)$ dato un certo $\overline{\lambda}$.
\end{osse}

\begin{prop}
Sia $A$ una matrice tridiagonale:
\[ A =
\left[ \begin{array}{ccccc}
a_1    & c_1    & 0      & \cdots & 0 \\
b_2    & a_2    & c_2    & \ddots & \vdots \\
0      & b_3    & \ddots & \ddots & 0 \\
\vdots & \ddots & \ddots & a_{n-1}& c_{n-1} \\
0      & \cdots & 0      & b_{n} & a_n
\end{array} \right],
\]
\begin{flushleft}
$d_0 :=  1$;   \\
$d_1 :=  a_1$; \\
$\vdots$      \\
$d_k :=  a_k d_{k-1} - b_kc_{k-1}d_{k-2}. \qquad (k = 2, \ldots, n)$,\\
\end{flushleft}
tale che $d_k \neq 0$ per $k = 1, \ldots , n$, allora $A$ è direttamente
fattorizzabile in $A = LU$.
\[ L =
\left[ \begin{array}{ccccc}
  1    & 0      & 0      & \cdots & 0 \\
b_2 \frac{d_0}{d_1}   &   1    & 0      & \ddots & \vdots \\
0      & b_3 \frac{d_1}{d_2}   & \ddots & \ddots & 0 \\
\vdots & \ddots & \ddots & 1      & 0 \\
0      & \cdots & 0      & b_{n}\frac{d_{n-2}}{d_{n-1}}   & 1
\end{array} \right],
\quad U =
\left[ \begin{array}{ccccc}
\frac{d_1}{d_0}   & c_1    & 0      & \cdots & 0 \\
0      & \frac{d_2}{d_1}    & c_2    & \ddots & \vdots \\
0      & 0      & \ddots & \ddots & 0 \\
\vdots & \ddots & \ddots & \frac{d_{n-1}}{d_{n-2}}& c_{n-1} \\
0      & \cdots & 0      & 0     & \frac{d_{n}}{d_{n-1}}
\end{array} \right].
\]
Sia $L$ che $U$ sono due matrici bidiagonali, il costo di costruzione è
``quasi'' gratuito poiché è dato dal solo colacolo dei $d_i$.
\end{prop}
\begin{dimo}$A \stackrel{?}{=} LU$.

%% TODO: finire la dimostrazione (esercizio)
E' sufficiente dimostrare che per ogni $k = 1, \ldots, n$:
\begin{itemize}
\item[-]$(LU)_{k,k} = a_k$.
  \begin{itemize}
  \item[]$k = 1$:\[ 1\cdot \frac{d_1}{d_0}=1\cdot \frac{a_1}{1} = a_1.\]
  \item[]$k = 2, \ldots, n$:
    \[
    b_k\frac{d_{k-1}}{d_{k-2}}\cdot c_{k-1} + 1 \cdot \frac{d_k}{d_{k-1}} =
    \frac{\cancel{b_kc_{k-1}d_{k-2}} + a_k\cancel{d_{k-1}}
    \cancel{- b_kc_{k-1}d_{k-2}}}{\cancel{d_{k-1}}} = a_k.
    \]
  \end{itemize}
\item[-]$(LU)_{k,k-1} = b_{k}$.
  \begin{itemize}
  \item[]$k = 2$:\[b_2\frac{d_0}{d_1}\cdot\frac{d_1}{d_0} = b_2\cdot 1 = b_2.\]
  \item[]$k = 3, \ldots, n$:
    \[
    \]
  \end{itemize}
\item[-]$(LU)_{k,k+1} = c_k$.
  \item[]$k = 1$:\[\]
  \item[]$k = 2, \ldots, n$:
    \[
    \]
\item[-]$(LU)_{i,j} = 0,\quad$ se $\ |i-j| \geq 2$.
  \item[]$k = 1$:\[\]
  \item[]$k = 2, \ldots, n$:
    \[
    \]
\end{itemize}

\end{dimo}

In questo caso la soluzione di $Ax = t$ è data da:
\[(LU)x = t \equiv \left\{
\begin{array}{l}
Ly = t \\
Ux = y.
\end{array}\right.\]
\begin{itemize}
\item[]$y_1 = t_1$;
\item[]$y_k = t_k - b_k \frac{d_{k-2}}{d_{k-1}}y_{k-1}, \qquad(k = 2,\ldots, n)$
\item[]$x_n = y_n\frac{d_{n-1}}{d_n}$,
\item[]$x_k = (y_k - c_kx_{k+1})\frac{d_{k-1}}{d_k}. \qquad(k = n-1, \ldots, 1)$
\end{itemize}

\section{Fattorizzazione LU diretta.}

Abbiamo visto dal teorema \cite{teo4.4} l'esistenza (ed unicità), sotto opportune ipotesi, di
una fattorizzazione $LU$ diretta di una matrice $A$. Ci preoccupiamo ora di costruire
tale fattorizzazione senza far uso dell'algoritmo di Gauss.

Siano $A$, $L$, $U$ del teorema precedente:

\[
A =
\left[
\begin{array}{ccc}
a_{1,1} & \cdots & a_{1,n} \\
\vdots & & \vdots \\
a_{n,1} & \cdots & a_{n,n}
\end{array}
\right], \qquad
\Delta_k \neq 0, \quad k = 1, \ldots, n.
\]

\[
L =
\left[
\begin{array}{cccc}
1 & 0 & \cdots & 0 \\
l_{2,1} & 1 & & \\
\vdots & & \ddots & \\
l_{n,1} & \cdots & l_{n, n-1} & 1
\end{array}
\right], \quad
U =
\left[
\begin{array}{cccc}
u_{1,1} & u_{1,2} & \cdots & u_{1,n} \\
0 & u_{2,2} & & u_{2,n} \\
\vdots & &\ddots & \vdots \\
0 & \cdots & 0 & u_{n,n}
\end{array}
\right].
\]

Si noti che la prima riga di $L$ è nota, da questo punto possiamo quindi
calcolare la prima riga di $U$. In questo modo abbiamo trovato la prima
colonna di $U$ e si può quindi calcolare la prima colonna di $L$.

Iterando il discorso è quindi possibile trovare la seconda riga di $U$ e
quindi la seconda colonna di $L$ e così via. Vediamo in concreto.

\begin{itemize}
\item[-]La prima riga di $L$ è nota: $\left[1\, 0\, \cdots\, 0\right]$, si può
calcolare la prima riga di $U$:
\[1 \cdot u_{1,i} = a_{1,i}. \quad (i = 1, \ldots, n) \]
\[\left[a_{1,1} \, a_{1,2} \, \cdots \, a_{1,n}\right]
=
\left[u_{1,1} \, u_{1,2} \, \cdots \, u_{1,n}\right].\]
\item[-]Ora, dalla prima colonna di $U$ calcoliamo la prima colonna di $L$:
\[l_{i,1} \cdot u_{1,1} = a_{i,1}. \quad (i = 1, \ldots, n)\]
\[
\overline{a}_1 = \left[
\begin{array}{c}
a_{1,1} \\
\vdots \\
a_{n,1}
\end{array}\right], \quad
\overline{u}_1 = \left[
\begin{array}{c}
u_{1,1} \\
0 \\
\vdots \\
0
\end{array}\right], \quad
\overline{l}_1 = \left[
\begin{array}{c}
\frac{a_{1,1}}{u_{1,1}} \\
\vdots \\
\frac{a_{n,1}}{u_{1,1}}
\end{array}\right].
\]

\item[-]Possiamo quindi fissare la seconda riga di $L$:
\[l_2 = \left[\frac{a_{2,1}}{u_{1,1}}\ 1 \ 0\ \cdots \ 0 \right].\]
e calcolare la seconda riga di $U$:
\[l_{2,i}\cdot u_{1,i} + 1 \cdot u_{2,i} = a_{2,i}. \quad (i = 1, \ldots, n) \]
\[\left[
l_{2,1} \ \cdots \ l_{2,n}
\right]
\left[
u_{1,1} \ \cdots \ u_{1,n}
\right] + 1\cdot \left[
u_{2,1} \ \cdots \ u_{2,n}
\right] = \left[
a_{2,1} \ \cdots \ a_{2,n}
\right].\]
\[u_{2,i} = a_{2,i} - l_{2,i}u_{1,i}. \quad (i = 1, \ldots, n)\]
\[u_2 =
\left[(a_{2,1}- l_{2,1}u_{1,1}) \ (a_{2,2} - 1\cdot u_{1,2}) \
(a_{2,3} - 0\cdot u_{1,2})\cdots \ (a_{2,n} -
0 \cdot u_{1,i})\right].
\]
\[u_2 =
\left[(a_{2,1}- l_{2,1}u_{1,1}) \ (a_{2,2} - 1\cdot u_{1,2}) \
(a_{2,3})\cdots \ (a_{2,n})\right].
\]
\item[-]E' ora possibile fissare la seconda colonna di $U$ e dedurre quindi
gli elementi della seconda colonna di $L$:
\[\overline{u}_2 =\left[
\begin{array}{c}
u_{1,2} \\
a_{2,2} - 1\cdot u_{1,2} \\
0 \\
\vdots \\
0
\end{array}\right].
\]

\[l_{i,1}u_{1,2} + \underbrace{l_{i,2}}_{\textrm{variabile}}\cdot\ u_{2,2} = a_{i,2}.\]
\[l_{i,2} = \frac{a_{i,2}- l_{i,1}u_{1,2}}{u_{2,2}}.\]
\end{itemize}

Generalizzando l'algoritmo si ottiene:

\begin{itemize}
\item[$\square$] $\quad k = 1, \ldots, n;$
    \begin{itemize}
    \item[o] $\qquad j = k, \ldots, n;$
      \[u_{k,j} = a_{k,j} - \sum_{p=1}^{k-1}l_{k,p}u_{p,j}.
      \]
    \item[o] $\qquad i = k+1, \ldots, n;$\\
      \[ l_{i,k} = \frac{\left[ a_{i,k} - \sum_{p = 1}^{k-1}l_{i,p}u_{p,k}\right]}
         {u_{k,k}}.\]
    \end{itemize}
\end{itemize}

Tale algoritmo, nella pratica, viene anche chiamato algoritmo \textbf{\emph{riga-colonna}}.

\begin{osse}
E' possibile salvare le matrici $L$ ed $U$ direttamente entro la matrice $A$. Basta procedere
per righe se si considerano le righe di $U$, e per colonne se si considerano le colonne di $L$.
\end{osse}

\begin{osse}
Il costo dell'algoritmo \emph{riga-colonna} è pari a $O(\frac{n}{3})$.
\end{osse}

Ricapitolando: se sappiamo che $A$ è non singolare allora sappiamo che esiste $P$ tale che
$PA = LU$. Ci chiediamo: quando $P \equiv I$? Nell'algoritmo di Gauss ad ogni passo in cui
non servono permutazioni, in quest'altro caso ogni volta che il minore è diverso da $0$.

