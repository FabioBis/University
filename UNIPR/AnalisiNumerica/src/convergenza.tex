\hyphenation{
Barrow
Binet
Chebyshev
Cholesky
Cramer
Gauss
Faber
Hausdorff
Householder
Laplace
Lebesque
Runghe
tras-for-ma-zio-ne
Torricelli
}

\chapter{Convergenza.}

\section{Teorema di Faber.}
Sia $[a,b] \subset \rr$, $f$ una funzione continua su tale intervallo ($f \in
\cc^0([a,b])$), prendiamo $n+1$ punti e costruiamo i polinomi interpolatori:

\[
\begin{array}{ll}
x_0^1 & p_0(x) \\
x_0^2 \ x_1^2 & p_1(x)\\
x_0^3 \ x_1^3 \ x_2^3 & p_2(x) \\
\vdots \\
x_0^{n+1} \ x_1^{n+1} \ \cdots \ x_n^{n+1} & p_n(x)
\end{array}
\]

Gli $x_i^{j+1}$ possono coincidere con gli $x_i^j$, ma in una data riga ogni
$x_i^k$ sono tutti distinti per ogni $i = 0, \ldots, k-1$.

\begin{flushleft}
Possiamo costruire una matrice infinita. Tuttavia Runghe ci porta a non essere
sicuri di questa situazione.
\[f(x) = \frac{1}{1+x^2}, \quad x \in [-5,5]\]
al crescere di $n$ abbiamo una divergenza sugli estremi dell'intervallo usando
i nodi e i polinomi interpolatori.
\end{flushleft}

Enunciamo un teorema che contiene questa idea.

\begin{teo}
Per ogni matrice interpolatoria esiste una funzione continua che \emph{non}
è il limite della successione dei polinomi $\{p_n(x)\}$.
\end{teo}
\begin{osse}
Con questo teorema non si afferma che non sia possibile costruire una
successione che converga alla funzione. Nell'esempio di Runghe si vedeva che,
sostituendo a nodi equidistanti i nodi di Chebyshev, la divergenza agli
estremi scompariva.

Per ogni funzione continua esiste una matrice interpolatoria che converge ad
una funzione continua. Il problema è diverso.
\end{osse}

\section{Polinomi di Chebyshev.}
Se $f\in \cc^1$ è utile utilizzare i nodi di Chebyshev (riportando $f \in
[-1,1]$) poiché otteniamo convergenza. Infatti:
\[
f(x) \in \cc^1([a,b]), \quad \|f(x) - p_n(x)\|_\infty =
\textrm{O}\left(\frac{1}{\sqrt{n}}\right).
\]
In cui $p_n(x)$ interpola $f$ sui nodi di Chebyshev.
\[
f, g: \qquad \underbrace{(f, g)_\omega}_{\textrm{scalare}}
= \int_{-1}^1 \underbrace{\frac{1}{\sqrt{1-x^2}}}_{\textrm{f.ne peso}}f(x)g(x)dx,
\qquad (f, g)_\omega = 0.
\]

\[
(p_n, p_m)_\omega = \int_{-1}^1\frac{1}{\sqrt{1-x^2}}p_n(x)p_m(x)dx.
\]
$(p_n, p_m)_\omega$ caratterizza l'ortogonalità nei polinomi di Chebyshev.

Ricordiamo che i nodi di Chebyshev sono gli \emph{zeri} del polinomio di
Chebyshev, quindi prima occorre costruire i polinomi, quindi calcolarne gli
zeri.

\subsection{Costruzione dei polinomi di Chebyshev.}
Come si trovano i polinomi di Chebyshev? Sappiamo che sono definiti
nell'intervallo $[-1,1]$.
\begin{flushleft}
$p_0(x) = 1$.\\
$p_1(x) = ax +b$.\\
\[
\omega(x) = \frac{1}{\sqrt{1-x^2}}.
\]

\[
(p_0, p_1)_\omega = \int_{-1}^1\frac{1}{\sqrt{1-x^2}}p_0(x)p_1(x)dx = 0.
\]
Questa è la prima condizione da imporre, poiché vogliamo dei polinomi
ortogonali, tuttavia non è sufficiente per avere entrambi i coefficienti $a$
e $b$.
\end{flushleft}

Se prendiamo un polinomio \emph{monico}\footnote{Un polinomio si dice monico
se il coefficiente della variabile di grado maggiore è pari a $1$ ($a_n=1$).}
 (è sufficiente dividere per $a$) gli zeri non cambiano.

\[p_1(x) = x + b'.\]
Questo ha una condizione ed una variabile, quindi lo determiniamo.

\[
p_2(x) = ax^2+bx+c.
\]
\[
\left\{
\begin{array}{l}(p_0,p_2)_\omega=0 \\(p_1,p_2)_\omega=0 \end{array}
\right. \ \longleftrightarrow \
\left\{
\begin{array}{l}\int_{-1}^1\omega(x)p_0(x)p_2(x)dx = 0\\
\int_{-1}^1\omega(x)p_1(x)p_2(x)dx = 0\end{array}
\right.
\]
Anche qui possiamo determinare il polinomio monico, due condizioni e due
incognite. E' sempre possibile costruire una famiglia di polinomi monici.

\begin{osse}
Cambiando la funzione \emph{peso} cambia la famiglia che si costruisce.
\end{osse}
Vediamo la seguente funzione peso:
\[
\omega(x) = \log(x). \quad \in [0,1]
\]

\begin{osse}
La funzione peso può avere problemi di integrazione (singolarità deboli) solo
in punti estremi e avere convergenza.
\end{osse}

Possiamo utilizzare Newton-Cotes?
\begin{itemize}
\item[$\bullet$]Chiuse? No perchè il logaritmo non è definito in $0$.
\item[$\bullet$]Aperte? Dipende da quanto vicino allo zero prendiamo i nodi.
Infatti nell'avvicinarsi a zero otteniamo un nodo del tutto sperimentale per
calcolare l'integrale. L'errore dipende dalle derivate.
\end{itemize}

Se costruiamo una formula di quadratura che \emph{assorba} la singolarità
debole della funzione peso possiamo analizzare solo la funzione $f$.

\section{Spline.}
L'idea di utilizzare le spline lineare è estendibile alle spline di altri
ordini?
\subsection{Spline cardinali.}
Sia $S_{3,\Delta}(x)$ una spline cubica interpolante vincolata
(completa),
\[\left.
\begin{array}{r}
S'_{3,\Delta}(x_0) = y'_0 = f'(x_0) \\
S'_{3,\Delta}(x_n) = y'_n = f'(x_n)
\end{array}\right\} \ \textrm{assegnate.}
\]
\[
\begin{array}{c}
S_{3,\Delta,-1}(x)\\
\vdots \\
S_{3,\Delta,i}(x) \\
\vdots \\
S_{3,\Delta,n+1}(x)
\end{array}
\]

\[
\begin{array}{c}S_{3,\Delta,i}(x) \\i = 0, \ldots, n. \end{array}= \left\{
\begin{array}{llr}
S_{3,\Delta,i}(x_j) = \delta_{i,j} & & j = 0, \ldots, n. \\
S'_{3,\Delta,i}(x_0) = 0 \\
S'_{3,\Delta,i}(x_n) = 0
\end{array}
\right.
\]
\[
S_{3,\Delta,-1}(x) = \left\{
\begin{array}{llr}
S_{3,\Delta,-1}(x_j) = 0 & & j = 0, \ldots, n. \\
S'_{3,\Delta,-1}(x_0) = 1 \\
S'_{3,\Delta,-1}(x_n) = 0
\end{array}
\right.
\]
\[
S_{3,\Delta,n+1}(x) = \left\{
\begin{array}{llr}
S_{3,\Delta,n+1}(x_j) = 0 & & j = 0, \ldots, n. \\
S'_{3,\Delta,n+1}(x_0) = 0 \\
S'_{3,\Delta,n+1}(x_n) = 1
\end{array}
\right.
\]

Le spline così definite si dicono \emph{cardinali}.

\[
S_{3,\Delta}(x) = \sum_{j=0}^nf(x_j)S_{3,\Delta,j}(x) + f'(x_0)S_{3,\Delta,-1}(x)
+f'(x_n)S_{3,\Delta,n+1}(x).
\]
\[
S_{3,\Delta}(x_i) = \sum_{j=0}^nf(x_j)S_{3,\Delta,j}(x_i) = f(x_i).
\]

\[
S'_{3,\Delta}(x) = \sum_{j=0}^nf(x_j)S'_{3,\Delta,j}(x) + f'(x_0)S'_{3,\Delta,-1}(x)
+f'(x_n)S'_{3,\Delta,n+1}(x).
\]
\[
S'_{3,\Delta}(x_0) =\cdots = f'(x_0).
\]
Soddisfa il primo vincolo, analogamente
\[
S'_{3,\Delta}(x_n) =\cdots = f'(x_n).
\]

\[
\Longrightarrow \ S_{3,\Delta}(x) \ \textrm{è una spline cubica interpolante
vincolata.}
\]
Questa nuova espressione di $S_{3,\Delta}(x)$ è simile all'interpolazione
lagrangiana. Infatti $S_{3,\Delta,j}(x)$ sono definiti su tutto l'intervallo
come i polinomi lagrangiani.

\subsection{Spline cubiche naturali.}
Come si costruisce una spline cubica naturale utilizzando le spline cardinali?

Innanzitutto non dobbiamo più avere dati aggiuntivi iniziali poiché la spline
è caratterizzata da:
\[
S''_{3,\Delta}(x_0) = S''_{3,\Delta}(x_n) = 0.
\]

Quindi rimangono questi soli vincoli:
\[
\begin{array}{c}S_{3,\Delta,i}(x) \\i = 0, \ldots, n. \end{array}= \left\{
\begin{array}{llr}
S_{3,\Delta,i}(x_j) = \delta_{i,j} & & j = 0, \ldots, n. \\
S'_{3,\Delta,i}(x_0) = 0 \\
S'_{3,\Delta,i}(x_n) = 0
\end{array}
\right.
\]

$S_{3,\Delta,-1}$ e $S_{3,\Delta,n+1}$ non servono più perchè non c'è più la
condizione di vincolo.

\[
S_{3,\Delta}(x) = \sum_{i=0}^nf(x_i)S_{3,\Delta,i}(x).
\]

Cosa disturba? L'errore locale si propaga su tutto  l'intervallo come in
Lagrange, perchè la base stessa si espande su tutto l'intervallo.

\subsubsection{Nuovo passo:}
occorre costruire una base di funzioni (cubiche) che abbiano supporto minimo
all'interno dell'intervallo.
\subsubsection{Suggerimento:}
si prendano come funzioni polinomi $B_i$ simili a quelli del capitolo
precedente ma di grado tre.

\begin{osse}
La dimensione dello spazio delle spline cubiche naturali è $n+1$ mentre quello
delle spline cubiche vincolate è $n+3$.
\end{osse}

\section{Tecnica dei minimi quadrati.}
\label{min-quad}
Prendiamo $n+1$ punti non necessariamente distinti e $n+1$ passaggi distinti:
\[
x_i \quad y_i \qquad i = 0, \ldots, n.
\]

Dato $m \ll n$, come si  ottiene il modello $f_m(x)$?

\begin{flushleft}
Sia $\varphi_0(x), \varphi_1(x), \ldots, \varphi_m(x)$ una base per lo spazio
in cui stiamo costruendo l'approssimazione di $x$.
\[
f(x) = c_0\varphi_0(x) + c_1\varphi_1(x) + \cdots + c_m\varphi_m(x).
\]
$c_j=$ ? $j = 0, \ldots, m$.
\end{flushleft}
Determinati gli elementi $c_i$ otteniamo il modello.

\[
r_i = y_i - f_m(x_i) \quad i = 0, \ldots n.
\]
$r_i$ si dice \emph{residuo}, ovvero quanto il modello differisce dal punto
dato.

\[
\sum_{i=0}^nr_i^2 = \sum_{i=0}^n\left(y_i - f_m(x_i)\right)^2
= \sum_{i=0}^n\left(y_i - \sum_{j=0}^mc_j\varphi_j(x_i)\right)^2.
\]

Occorre cercarne il minimo:
\[
\min\sum_{i=0}^nr_i^2 = \min_{c \in \rr^{m+1}} \sum_{i=0}^n\left(y_i -
\sum_{j=0}^mc_j\varphi_j(x_i)\right)^2 \qquad c = \left[\begin{array}c_0\\
c_1 \\ \vdots \\ c_m \end{array}\right],
\]

$y_i = f_m(x_i) + r_i = \sum_{j=0}^mc_j\varphi_j(x_i), \quad i = 0,\ldots, n$.

\[
\left[ \begin{array}{c}y_0\\y_1\\\vdots\\y_n\end{array}\right] =
\underbrace{\left[ \begin{array}{cccc}
\varphi_0(x_0) & \varphi_1(x_0) & \cdots & \varphi_m(x_0) \\
\varphi_0(x_1) & \varphi_1(x_1) & \cdots & \varphi_m(x_1) \\
\vdots \\
\varphi_0(x_n) & \varphi_1(x_n) & \cdots & \varphi_m(x_n)
\end{array}\right]}_{A \in \rr^{n+1 \times m+1}: \ \textrm{rettangolare}}
\left[ \begin{array}{c}c_0\\c_1\\\vdots\\c_n\end{array}\right]+
\left[ \begin{array}{c}r_0\\r_1\\\vdots\\r_n\end{array}\right]
\]

$Ac = y$ (è il minimo).

\begin{flushleft}
Condizione necessaria per trovare il minimo:
\[
\frac{\partial\sum_{i=0}^nr_i^2}{\partial c_k}= 0, \quad
\frac{\partial\xi^2}{\partial c_k} = 0. \qquad k = 0, \ldots,m.
\]
\[
\textrm{se} \quad \sum_{i=0}^nr_i^2 = \xi^2.
\]
\end{flushleft}

\[
\frac{\partial}{\partial c_k}\sum_{i=0}^n\left(y_i - \sum_{j=0}^mc_j
\varphi_j(x_i)\right)^2 = 0. \qquad k = 0, \ldots,m.
\]

\[
\cancel{2}\sum_{i=0}^n\left(y_i - \sum_{j=0}^mc_j
\varphi_j(x_i)\right) = 0.
\]

\[
\sum_{i=0}^ny_i\varphi_k(x_i) -  \sum_{i=0}^n\sum_{j=0}^mc_j
\varphi_j(x_i)\varphi_k(x_i) = 0.
\]

\[
\sum_{j=0}^mc_j\sum_{i=0}^n\varphi_j(x_i)\varphi_k(x_i) =
\sum_{i=0}^ny_i\varphi_k(x_i). \qquad  k = 0, \ldots,m.
\]

Vediamo cosa significano:
\[
k = 0, \quad \sum_{j=0}^mc_j\sum_{i=0}^n\varphi_j(x_i)\varphi_0(x_i) =
\sum_{i=0}^ny_i\varphi_0(x_i).
\]
$k = 0\ \wedge\ j = 0 \ \longrightarrow \sum_{i=0}^n\varphi_0(x_i)
\varphi_0(x_i)$ etc.
\[
\begin{array}{c}k = 0 \\\\ k = 1 \\\\ \vdots \\\\ k = m\end{array}
\underbrace{\left[\begin{array}{cccc}
\sum_{i=0}^n\varphi_0(x_i)\varphi_0(x_i)
& \sum_{i=0}^n\varphi_1(x_i)\varphi_0(x_i) & \cdots
& \sum_{i=0}^n\varphi_m(x_i)\varphi_0(x_i) \\\\
\sum_{i=0}^n\varphi_0(x_i)\varphi_1(x_i)
& \sum_{i=0}^n\varphi_1(x_i)\varphi_1(x_i) & \cdots
& \sum_{i=0}^n\varphi_m(x_i)\varphi_1(x_i) \\\\
\vdots  & & \ddots\\\\
\sum_{i=0}^n\varphi_0(x_i)\varphi_m(x_i)
& \sum_{i=0}^n\varphi_1(x_i)\varphi_m(x_i) & \cdots
& \sum_{i=0}^n\varphi_m(x_i)\varphi_m(x_i)
\end{array}\right]}_{B \in \rr^{m+1 \times m+1}:\ \textrm{quadrata}}
\]

\[
d = \left[
\begin{array}{c}
\sum_{i=0}^ny_i\varphi_0(x_i)\\
\vdots \\
\sum_{i=0}^ny_i\varphi_m(x_i)
\end{array}
\right], \quad
c = \left[
\begin{array}{c}
c_0 \\
\vdots \\
c_m
\end{array}
\right].
\]

$d \in \rr^{m+1}$ è il vettore dei termini noti, mentre $c \in \rr^{m+1}$ è
il vettore delle incognite. Dobbiamo quindi risolvere il sistema $Bc = d$.

\begin{osse}
$B = A^tA$ e $d = A^ty$, da queste relazioni otteniamo:

\[
Ac = y \ \longrightarrow \begin{array}{c}A^tAc = A^ty \\ Bc = d\end{array}.
\]
\end{osse}

Poiché $\varphi_0, \varphi_1, \ldots, \varphi_n$ sono linearmente
indipendenti la matrice $A$ ha rango \emph{massimo}, ovvero $n+1$. Non
possiamo parlare di singolarità perchè la matrice è rettangolare.

Ne segue che $B$ è \emph{non singolare}.

\begin{flushleft}
Possiamo utilizzare la tecnica gaussiana per risolvere il sistema, ma essendo
$B$ simmetrica, se riuscissimo a dimostrare che è anche definita positiva
potremmo applicare la fattorizzazione di Cholesky, riducendo il costo
computazionale.
\end{flushleft}
$\left(A^tA\right)^t = A^tA$.
\[
\forall x \in \rr^{m+1} \quad x^tA^tAx > 0, \ \left(Ax\right)^tAx > 0.
\]
E' definita positiva essendo questo un prodotto scalare.

\begin{flushleft}
La tecnica dei minimi quadrati che minimizza quel funzionale al quadrato si
riduce alla soluzione di un sistema lineare mediante Cholesky.
\end{flushleft}

\begin{exe}Retta ai minimi quadrati.
\[x_i\quad y_i\]
$f(x) = ax +b \quad = c_0 + c_1x$\\
$a, b =$? \\
$\varphi_0(x) = 1$.\\
$\varphi_1(x) = x$. \\
\[
A = \begin{array}{cc}
\left[\begin{array}{c}1 \\ 1 \\ \vdots \\ 1\end{array}\right.
&
\left.\begin{array}{c}x_0\\x_1\\\vdots\\x_n\end{array}\right]
\\
\substack{\phantom{A}\uparrow \\\phantom{Ab}\varphi_0(x_i)}
& \substack{\uparrow\phantom{Ab} \\\phantom{}\varphi_0(x_i)}
\end{array} \quad
B = \underbrace{
\left[\begin{array}{cc}\phantom{\varphi_0(x)} & \phantom{\varphi_0(x)}\\\\
\\
\phantom{\varphi_0(x)} & \phantom{\varphi_0(x)}
\end{array}\right]}_{\substack{2 \times 2 \\ \textrm{qualunque sia} \\ \textrm{il n° dei
nodi}}} \quad c =
\left[\begin{array}{c}c_0\\c_1\end{array}\right]\quad
d = A^t \cdot \left[\begin{array}{c}y_0\\y_1\end{array}\right].
\]
\end{exe}

Esiste un'alternativa a questo modo di procedere più corretta e più stabile?
\[
\xi^2 = \sum_{i=0}^nr_i^2.
\]

\[
\min \xi^2 =
\min_c \sum_{i=0}^n\left(y_i - \sum_{j=0}^mc_j\varphi_j(x_i)\right)^2 =
\min_c \left\|y - Ac \right\|_2^2.
\]

Ma $A$ non è una matrice quadrata, occorre quindi ripercorrere la stessa
``strada'' utilizzata per arrivare alla fattorizzazione delle matrici $QR$
nel caso rettangolare. Quindi utilizziamo la fattorizzazione $QR$ su A.
\[A = QR.\]

\subsubsection{Domanda:}E' applicabile questa tecnica a $f(x) = c_0 +c_1
e^{c_3x}$?
\begin{flushleft}
La risposta è no, perchè questo non è un problema lineare nelle incognite.
\end{flushleft}
Invece in $f(x) = c_0 + c_1e^x$ la tecnica è applicabile perchè avremmo
$\varphi_0 = 1$ e $\varphi_1 = e^x$ ovvero un problema lineare nelle due
incognite $c_0$ e $c_1$.

\begin{teo}
Sia $f(x) \in \cc^4([a,b])$, $S_{3,\Delta}(x)$ una spline cubica vincolata
(completa) relativa ad una decomposizione $\Delta$ di $[a,b]$ e interpolante
la funzione $f(x)$. Indicato con $M, H$ e $h$:
\[M = \max_{a \leq x \leq b} \left|f^{(iv)}(x)\right|, \ H = \max_{1 \leq i \leq n}
h_i, \ h = \min_{1 \leq i \leq n}h_i;\]
allora si ha:
\begin{enumerate}
\item $\left|f(x) - S_{3,\Delta}(x)\right| \leq \frac{7}{8}M\frac{H^5}{h}$.
\begin{osse}
Se il passo è costante si ha $H = h$ per cui:
\[\left|f(x) - S_{3,\Delta}(x)\right| \leq \frac{7}{8}MH^4,\]
ovvero una convergenza dell'ordine di $H^4$.
\end{osse}
\[H \to 0 \ \Longrightarrow \ f(x) \to S_{3,\Delta}(x).\]
\item $\left|f(x)' - S'_{3,\Delta}(x)\right| \leq \frac{7}{4}M\frac{H^4}{h}$.\\
Analogamente a prima se il passo è costante abbiamo una convergenza
dell'ordine di $H^3$.
\item $\left|f''(x) - S''_{3,\Delta}(x)\right| \leq \frac{7}{4}M\frac{H^3}{h}$.
\\
Quindi per approssimare le funzioni derivate della $f$ basta derivare la
spline che la approssima.

\item$\left|f'''(x) - S'''_{3,\Delta}(x)\right| \leq \frac{7}{2}M\frac{H^2}{h}$.
 \\ $x \in (x_{i-1}, x_i) \quad i = 1, \ldots, n$, perchè $S \notin \cc^\infty$
nei bordi.
\end{enumerate}

\[
b_1-a_1 = \frac{b_0 - a_0}{2}, \quad b_2-a_2 = \frac{b_1-a_1}{2} =
\frac{b_0 - a_0}{2^2}.
\]
\end{teo}

